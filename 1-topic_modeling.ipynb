{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1256c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### static variables\n",
    "\n",
    "COLUMNS_DOCS = [\n",
    "    \"doc_id\",\n",
    "    \"language\",\n",
    "    \"domain\",\n",
    "    \"content\",\n",
    "    \"company_name\",\n",
    "    \"court_name\",\n",
    "    \"hospital_patient_name\",\n",
    "]\n",
    "\n",
    "COLUMNS_DOCS_MANIPULATED_TEXTUAL = [\n",
    "    *COLUMNS_DOCS,\n",
    "    \"original_doc_id\",\n",
    "]\n",
    "\n",
    "COLUMNS_DOCS_MANIPULATED_TABULAR = [\n",
    "    \"doc_id\",\n",
    "    \"language\",\n",
    "    \"domain\",\n",
    "    \"content\",\n",
    "    \"company_names\",\n",
    "    \"court_names\",\n",
    "    \"hospital_patient_names\",\n",
    "    \"original_doc_ids\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd890fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper functions\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "def get_documents() -> pd.DataFrame:\n",
    "    docs_original = pd.read_csv(\"data/DRAGONball/en/docs.csv\", usecols=[\"doc_id\", \"domain\", \"content\"])\n",
    "    docs_manipulated_single_textual = pd.read_csv(\n",
    "        \"data/additional_data/docs/textual_manipulations_result.csv\",\n",
    "        usecols=[\"doc_id\", \"domain\", \"content\", \"original_doc_id\"],\n",
    "        dtype={\"original_doc_id\": \"Int64\"},\n",
    "    )\n",
    "    docs_manipulated_single_tabular = pd.read_csv(\n",
    "        \"data/additional_data/docs/tabular_manipulations_result.csv\",\n",
    "        usecols=[\"doc_id\", \"domain\", \"content\", \"original_doc_ids\"],\n",
    "        converters={\"original_doc_ids\": ast.literal_eval},\n",
    "    )\n",
    "    docs_manipulated_multi_textual = pd.read_csv(\n",
    "        \"data/additional_data/docs/multi_textual_manipulations.csv\",\n",
    "        usecols=[\"doc_id\", \"domain\", \"content\", \"original_doc_id\"],\n",
    "        dtype={\"original_doc_id\": \"Int64\"},\n",
    "    )\n",
    "    print(f\"# original docs: {len(docs_original)}\")\n",
    "    print(f\"# manipulated textual docs: {len(docs_manipulated_single_textual)}\")\n",
    "    print(f\"# manipulated tabular docs: {len(docs_manipulated_single_tabular)}\")\n",
    "    print(f\"# manipulated textual multi docs: {len(docs_manipulated_multi_textual)}\")\n",
    "\n",
    "    return pd.concat(\n",
    "        [\n",
    "            docs_original,\n",
    "            docs_manipulated_single_textual,\n",
    "            docs_manipulated_multi_textual,\n",
    "            docs_manipulated_single_tabular,\n",
    "        ],\n",
    "        sort=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26b458",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['original_doc_id']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs_df = \u001b[43mget_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m docs_list = docs_df[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].to_list()\n\u001b[32m      5\u001b[39m tfidf_vectorizer = TfidfVectorizer(max_df=\u001b[32m0.95\u001b[39m, min_df=\u001b[32m1\u001b[39m, stop_words=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mget_documents\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m docs_manipulated_single_textual = pd.read_csv(\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata/additional_data/docs/textual_manipulations_result.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     usecols=[\u001b[33m\"\u001b[39m\u001b[33mdoc_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moriginal_doc_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     18\u001b[39m     dtype={\u001b[33m\"\u001b[39m\u001b[33moriginal_doc_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mInt64\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m docs_manipulated_single_tabular = pd.read_csv(\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata/additional_data/docs/tabular_manipulations_result.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     usecols=[\u001b[33m\"\u001b[39m\u001b[33mdoc_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moriginal_doc_ids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     23\u001b[39m     converters={\u001b[33m\"\u001b[39m\u001b[33moriginal_doc_ids\u001b[39m\u001b[33m\"\u001b[39m: ast.literal_eval},\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m docs_manipulated_multi_textual = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/additional_data/docs/multi_textual_manipulations.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoc_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdomain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moriginal_doc_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moriginal_doc_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mInt64\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# original docs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs_original)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# manipulated textual docs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs_manipulated_single_textual)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:140\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.orig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.usecols_dtype == \u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols).issubset(\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_names\n\u001b[32m    139\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.names) > \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/pandas/io/parsers/base_parser.py:979\u001b[39m, in \u001b[36mParserBase._validate_usecols_names\u001b[39m\u001b[34m(self, usecols, names)\u001b[39m\n\u001b[32m    977\u001b[39m missing = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    980\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    981\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    982\u001b[39m     )\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[31mValueError\u001b[39m: Usecols do not match columns, columns expected but not found: ['original_doc_id']"
     ]
    }
   ],
   "source": [
    "docs_df = get_documents()\n",
    "docs_list = docs_df[\"content\"].to_list()\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, stop_words=\"english\")\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(docs_list)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Size of vocabulary: {len(tfidf_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NMF (perfect condictions)\n",
    "# nmf = NMF(n_components=108, init=\"nndsvda\", max_iter=400)\n",
    "# nmf = nmf.fit(tfidf_features[:108])\n",
    "# nmf_data = nmf.transform(tfidf_features)\n",
    "# nmf_data_normalised = normalize(nmf_data, norm=\"l1\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NMF (only no. of topics known)\n",
    "nmf = NMF(n_components=108, init=\"nndsvda\", max_iter=400)\n",
    "nmf_data = nmf.fit_transform(tfidf_features)\n",
    "nmf_data_normalised = normalize(nmf_data, norm=\"l1\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSA\n",
    "# lsa = TruncatedSVD(n_components=108)\n",
    "# lsa_data = lsa.fit_transform(tfidf_features)\n",
    "# lsa_data_normalised = normalize(lsa_data, norm=\"l2\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6592352",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA\n",
    "# lda = LatentDirichletAllocation(n_components=108)\n",
    "# lda_data_normalised = lda.fit_transform(tfidf_features, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2553110",
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper functions (1/2)\n",
    "def merge_original_ids(row):\n",
    "    if isinstance(row[\"original_doc_ids_tmp\"], list):\n",
    "        return row[\"original_doc_ids_tmp\"]\n",
    "    elif pd.notna(row[\"original_doc_id_tmp\"]):\n",
    "        return [row[\"original_doc_id_tmp\"]]\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "\n",
    "def calc_topics(row):\n",
    "    if isinstance(row[\"original_doc_ids\"], list):\n",
    "        if len(row[\"original_doc_ids\"]) > 1:\n",
    "            return np.argsort(row[\"doc_vector\"])[-10:][::-1].tolist()\n",
    "    return [np.argmax(row[\"doc_vector\"])]\n",
    "\n",
    "\n",
    "def calc_topics_for_cumulative_threshold(row, threshold=0.9):\n",
    "    sorted_indices = np.argsort(row)[::-1]\n",
    "\n",
    "    # Sort the probabilities accordingly\n",
    "    sorted_probs = row[sorted_indices]\n",
    "\n",
    "    # Compute cumulative sum\n",
    "    cumulative = np.cumsum(sorted_probs)\n",
    "\n",
    "    # Find the cutoff index where cumulative sum first exceeds threshold\n",
    "    cutoff = np.searchsorted(cumulative, threshold)\n",
    "\n",
    "    # Select the indices up to and including that point\n",
    "    selected_indices = sorted_indices[: cutoff + 1]\n",
    "\n",
    "    return selected_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper functions (2/2)\n",
    "def calc_topic_hitrate(row):\n",
    "    if not isinstance(row[\"original_doc_ids\"], list):\n",
    "        return None\n",
    "\n",
    "    original_doc_ids: List[int] = row[\"original_doc_ids\"]\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for id in original_doc_ids:\n",
    "        topics_row = set(row[\"topics\"])\n",
    "        original_row = docs.loc[docs[\"doc_id\"].astype(int) == int(id)].iloc[0]\n",
    "        topics_original = set(original_row[\"topics\"])\n",
    "        res.append(len(topics_row.intersection(topics_original)) > 0)\n",
    "\n",
    "    return np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4cd9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 56.67 %\n",
      "Recall on non-tabular docs: 85.0 %\n",
      "Average comparisons to make: 13.87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_vector</th>\n",
       "      <th>original_doc_ids</th>\n",
       "      <th>topics</th>\n",
       "      <th>topics_hitrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>100134</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...</td>\n",
       "      <td>[134]</td>\n",
       "      <td>[9]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>100136</td>\n",
       "      <td>[0.0, 0.0020720391791662052, 0.004018328717750...</td>\n",
       "      <td>[136]</td>\n",
       "      <td>[5, 11, 104, 94, 98, 9, 30, 78, 89]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>100139</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[139]</td>\n",
       "      <td>[11]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>100046</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.170...</td>\n",
       "      <td>[46]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>100047</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[47]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>400110</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...</td>\n",
       "      <td>[110]</td>\n",
       "      <td>[45, 105, 54, 103]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>400116</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[116]</td>\n",
       "      <td>[70, 36, 105, 54]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>300001</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.004799967831828898, 0.0...</td>\n",
       "      <td>[46, 47, 52, 59, 66, 71, 72, 77, 78, 79]</td>\n",
       "      <td>[26]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>300002</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[134, 136, 139, 112, 114, 115, 119, 123, 125, ...</td>\n",
       "      <td>[37]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>300003</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[179, 181, 198, 199, 208, 209, 207, 205, 212, ...</td>\n",
       "      <td>[39]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id                                         doc_vector  \\\n",
       "108  100134  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...   \n",
       "109  100136  [0.0, 0.0020720391791662052, 0.004018328717750...   \n",
       "110  100139  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "111  100046  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.170...   \n",
       "112  100047  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "..      ...                                                ...   \n",
       "166  400110  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...   \n",
       "167  400116  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "168  300001  [0.0, 0.0, 0.0, 0.0, 0.004799967831828898, 0.0...   \n",
       "169  300002  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "170  300003  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                      original_doc_ids  \\\n",
       "108                                              [134]   \n",
       "109                                              [136]   \n",
       "110                                              [139]   \n",
       "111                                               [46]   \n",
       "112                                               [47]   \n",
       "..                                                 ...   \n",
       "166                                              [110]   \n",
       "167                                              [116]   \n",
       "168           [46, 47, 52, 59, 66, 71, 72, 77, 78, 79]   \n",
       "169  [134, 136, 139, 112, 114, 115, 119, 123, 125, ...   \n",
       "170  [179, 181, 198, 199, 208, 209, 207, 205, 212, ...   \n",
       "\n",
       "                                  topics  topics_hitrate  \n",
       "108                                  [9]             1.0  \n",
       "109  [5, 11, 104, 94, 98, 9, 30, 78, 89]             1.0  \n",
       "110                                 [11]             1.0  \n",
       "111                                 [23]             1.0  \n",
       "112                                 [10]             1.0  \n",
       "..                                   ...             ...  \n",
       "166                   [45, 105, 54, 103]             1.0  \n",
       "167                    [70, 36, 105, 54]             1.0  \n",
       "168                                 [26]             0.0  \n",
       "169                                 [37]             0.0  \n",
       "170                                 [39]             0.0  \n",
       "\n",
       "[63 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### evaluate method\n",
    "import importlib\n",
    "from utils import evaluation\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "\n",
    "transformed_data = nmf_data_normalised\n",
    "\n",
    "docs = pd.DataFrame(\n",
    "    {\n",
    "        \"doc_id\": docs_df[\"doc_id\"].to_list(),\n",
    "        \"original_doc_id_tmp\": docs_df[\"original_doc_id\"].to_list(),\n",
    "        \"original_doc_ids_tmp\": docs_df[\"original_doc_ids\"].to_list(),\n",
    "        \"doc_vector\": list([doc for doc in transformed_data]),\n",
    "    }\n",
    ")\n",
    "\n",
    "docs[\"original_doc_ids\"] = docs.apply(merge_original_ids, axis=1)\n",
    "docs = docs.drop([\"original_doc_id_tmp\", \"original_doc_ids_tmp\"], axis=1)\n",
    "docs[\"topics\"] = docs[\"doc_vector\"].apply(calc_topics_for_cumulative_threshold, args=(0.95,))\n",
    "\n",
    "# docs[\"len(topics)\"] = docs[\"topics\"].apply(len)\n",
    "# docs[\"topic_hitrate\"] = docs.apply(calc_topic_hitrate, axis=1)\n",
    "# docs[\"num_non-zeros_in_vector\"] = docs[\"doc_vector\"].apply(lambda v: sum(i > 0 for i in v))\n",
    "\n",
    "# print(f\"Avg. number of topics: {round(docs[\"len(topics)\"].mean(), 2)}\")\n",
    "# docs[\"doc_vector\"] = docs[\"doc_vector\"].apply(lambda v: np.sort(v)[::-1]).apply(lambda v: [round(i, 4) for i in v])\n",
    "\n",
    "\n",
    "docs, recall = evaluation.evaluate_clusters(docs, \"topics\")\n",
    "filtered_docs = docs.loc[docs[\"original_doc_ids\"].notna()]\n",
    "print(f\"Recall: {round(recall * 100, 2)} %\")\n",
    "print(f\"Recall on non-tabular docs: {round(filtered_docs[\"topics_hitrate\"][:-3].mean() * 100, 2)} %\")\n",
    "print(f\"Average comparisons to make: {evaluation.count_avg_related_docs(docs, \"topics\"):.2f}\")\n",
    "filtered_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
