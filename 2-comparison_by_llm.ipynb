{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec48a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from utils import io_helpers\n",
    "from utils import llm\n",
    "\n",
    "importlib.reload(io_helpers)\n",
    "importlib.reload(llm)\n",
    "\n",
    "FILEPATH = \"evaluation/_pairwise_comparison_bool.csv\"\n",
    "FIELDNAMES = [\"id1\", \"id2\", \"conflicts\", \"model\", \"prompt\"]\n",
    "\n",
    "\n",
    "def get_known_results() -> pd.DataFrame:\n",
    "    try:\n",
    "        known_results = pd.read_csv(\n",
    "            FILEPATH,\n",
    "            usecols=FIELDNAMES,\n",
    "            dtype={\"id1\": \"Int64\", \"id2\": \"Int64\", \"conflicts\": \"boolean\"},\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(os.path.dirname(FILEPATH), exist_ok=True)\n",
    "        with open(FILEPATH, \"x\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, FIELDNAMES)\n",
    "            writer.writeheader()\n",
    "        known_results = pd.DataFrame(columns=FIELDNAMES)\n",
    "    return known_results\n",
    "\n",
    "\n",
    "def pairs_generator(doc_id: int, documents: pd.DataFrame):\n",
    "    doc = documents[documents[\"doc_id\"].astype(int) == int(doc_id)].squeeze()\n",
    "    related_docs = documents[documents[\"doc_id\"].isin(doc[\"related_docs\"])]\n",
    "\n",
    "    for _, related_doc in related_docs.iterrows():\n",
    "        id1 = doc[\"doc_id\"]\n",
    "        id2 = related_doc[\"doc_id\"]\n",
    "        expected_result = (\n",
    "            (id2 in doc[\"original_doc_ids\"])\n",
    "            | (id1 in related_doc[\"original_doc_ids\"])\n",
    "            | (\n",
    "                len(set(doc[\"original_doc_ids\"]).intersection(set(related_doc[\"original_doc_ids\"]))) > 0\n",
    "            )  # share the same original_doc\n",
    "        )\n",
    "        yield (id1, id2, expected_result)\n",
    "\n",
    "\n",
    "def compare_and_extract_llm(\n",
    "    doc_id1: int,\n",
    "    doc_id2: int,\n",
    "    expected_result: bool,\n",
    "    model: str,\n",
    "    documents: pd.DataFrame,\n",
    "    prompts: object,\n",
    "    expect_extraction: bool = False,\n",
    "):\n",
    "    known_results = get_known_results()\n",
    "    if not known_results[\n",
    "        (\n",
    "            (known_results[\"id1\"] == doc_id1) & (known_results[\"id2\"] == doc_id2)\n",
    "            | ((known_results[\"id1\"] == doc_id2) & (known_results[\"id2\"] == doc_id1))\n",
    "        )\n",
    "        & (known_results[\"model\"] == model)\n",
    "    ].empty:\n",
    "        print(f\"-- {doc_id1} & {doc_id2} -- processed before\")\n",
    "        return\n",
    "\n",
    "    system_prompt = prompts[\"system_prompt\"]\n",
    "\n",
    "    text1 = documents[documents[\"doc_id\"].astype(int) == int(doc_id1)].squeeze()[\"content\"]\n",
    "    text2 = documents[documents[\"doc_id\"].astype(int) == int(doc_id2)].squeeze()[\"content\"]\n",
    "\n",
    "    user_prompt = llm.format_user_prompt(prompts[\"user_prompt\"], text1=text1, text2=text2)\n",
    "\n",
    "    response_format = (\n",
    "        llm.LLMDocumentComparisonExtractResponse if expect_extraction else llm.LLMDocumentComparisonCheckResponse\n",
    "    )\n",
    "    llm_response = llm.call_any_llm(system_prompt, user_prompt, model, response_format_pydantic=response_format)\n",
    "    actual_result = llm_response.contradictory_info_found\n",
    "\n",
    "    print(f\"-- {doc_id1} & {doc_id2} -- \", end=\"\")\n",
    "    if expected_result == actual_result:\n",
    "        print(colored(\"Check!\", \"green\"))\n",
    "    else:\n",
    "        print(colored(\"Wrong!\", \"red\"))\n",
    "        print(f\"expected result: {expected_result}\")\n",
    "        print(f\"actual result: {actual_result}\")\n",
    "\n",
    "    new_row = {\n",
    "        \"id1\": doc_id1,\n",
    "        \"id2\": doc_id2,\n",
    "        \"conflicts\": actual_result,\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompts[\"user_prompt\"],\n",
    "    }\n",
    "    known_results = pd.concat([known_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    known_results.to_csv(FILEPATH, columns=FIELDNAMES, mode=\"w\", index=False)\n",
    "\n",
    "    if expect_extraction:\n",
    "        return llm_response.contradictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3246ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from utils import io_helpers\n",
    "from utils import llm\n",
    "import ast\n",
    "\n",
    "importlib.reload(io_helpers)\n",
    "importlib.reload(llm)\n",
    "\n",
    "FILEPATH = \"evaluation/_pairwise_comparison.csv\"\n",
    "FIELDNAMES = [\"model\", \"comparison_type\", \"id1\", \"id2\", \"contain_conflicts\", \"conflicting_passages\"]\n",
    "\n",
    "\n",
    "def get_known_results() -> pd.DataFrame:\n",
    "    try:\n",
    "        known_results = pd.read_csv(\n",
    "            FILEPATH,\n",
    "            usecols=FIELDNAMES,\n",
    "            dtype={\"id1\": \"Int64\", \"id2\": \"Int64\", \"contain_conflicts\": \"boolean\"},\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(os.path.dirname(FILEPATH), exist_ok=True)\n",
    "        with open(FILEPATH, \"x\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, FIELDNAMES)\n",
    "            writer.writeheader()\n",
    "        known_results = pd.DataFrame(columns=FIELDNAMES)\n",
    "    return known_results\n",
    "\n",
    "\n",
    "def compare_and_extract_llm(\n",
    "    doc_id1: int, doc_id2: int, model: str, comparison_type: str, documents: pd.DataFrame, prompts: object\n",
    "):\n",
    "    known_results = get_known_results()\n",
    "    if not known_results[\n",
    "        (\n",
    "            ((known_results[\"id1\"] == doc_id1) & (known_results[\"id2\"] == doc_id2))\n",
    "            | ((known_results[\"id1\"] == doc_id2) & (known_results[\"id2\"] == doc_id1))\n",
    "        )\n",
    "        & (known_results[\"model\"] == model)\n",
    "    ].empty:\n",
    "        print(f\"-- {doc_id1} & {doc_id2} -- processed before\")\n",
    "        return\n",
    "\n",
    "    system_prompt = prompts[\"system_prompt\"]\n",
    "\n",
    "    text1 = documents[documents[\"doc_id\"].astype(int) == int(doc_id1)].squeeze()[\"content\"]\n",
    "    text2 = documents[documents[\"doc_id\"].astype(int) == int(doc_id2)].squeeze()[\"content\"]\n",
    "\n",
    "    user_prompt = llm.format_user_prompt(prompts[\"user_prompt\"], text1=text1, text2=text2)\n",
    "\n",
    "    print(f\"-- {doc_id1} & {doc_id2} -- \", end=\"\")\n",
    "    llm_response: llm.LLMDocumentComparisonExtractResponse = llm.call_any_llm(\n",
    "        system_prompt, user_prompt, model, response_format_pydantic=llm.LLMDocumentComparisonExtractResponse\n",
    "    )\n",
    "    print(\"Done\")\n",
    "\n",
    "    new_row = {\n",
    "        \"model\": model,\n",
    "        \"comparison_type\": comparison_type,\n",
    "        \"id1\": doc_id1,\n",
    "        \"id2\": doc_id2,\n",
    "        \"contain_conflicts\": llm_response.contradictory_info_found,\n",
    "        \"conflicting_passages\": llm_response.contradictions,\n",
    "    }\n",
    "    known_results = pd.concat([known_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    known_results.to_csv(FILEPATH, columns=FIELDNAMES, mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41fe9754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>content</th>\n",
       "      <th>original_doc_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Acme Government Solutions is a government indu...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Entertainment Enterprises Inc. is an entertain...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Advanced Manufacturing Solutions Inc., establi...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>Finance</td>\n",
       "      <td>EcoGuard Solutions, established on April 15, 2...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Green Fields Agriculture Ltd., established on ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>400116</td>\n",
       "      <td>Law</td>\n",
       "      <td>In a significant legal proceeding at the Cedar...</td>\n",
       "      <td>[116]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>400059</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Retail Emporium, a well-established retail gia...</td>\n",
       "      <td>[59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300001</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Changes that occurred in senior management of ...</td>\n",
       "      <td>[46, 47, 52, 59, 66, 71, 72, 77, 78, 79]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300002</td>\n",
       "      <td>Law</td>\n",
       "      <td>Chief judge according to the court judgment of...</td>\n",
       "      <td>[134, 136, 139, 112, 114, 115, 119, 123, 125, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300003</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Past disease history of Y. Evans according to ...</td>\n",
       "      <td>[179, 181, 198, 199, 208, 209, 207, 205, 212, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id   domain                                            content  \\\n",
       "0       40  Finance  Acme Government Solutions is a government indu...   \n",
       "1       41  Finance  Entertainment Enterprises Inc. is an entertain...   \n",
       "2       42  Finance  Advanced Manufacturing Solutions Inc., establi...   \n",
       "3       43  Finance  EcoGuard Solutions, established on April 15, 2...   \n",
       "4       44  Finance  Green Fields Agriculture Ltd., established on ...   \n",
       "..     ...      ...                                                ...   \n",
       "28  400116      Law  In a significant legal proceeding at the Cedar...   \n",
       "29  400059  Finance  Retail Emporium, a well-established retail gia...   \n",
       "0   300001  Finance  Changes that occurred in senior management of ...   \n",
       "1   300002      Law  Chief judge according to the court judgment of...   \n",
       "2   300003  Medical  Past disease history of Y. Evans according to ...   \n",
       "\n",
       "                                     original_doc_ids  \n",
       "0                                                  []  \n",
       "1                                                  []  \n",
       "2                                                  []  \n",
       "3                                                  []  \n",
       "4                                                  []  \n",
       "..                                                ...  \n",
       "28                                              [116]  \n",
       "29                                               [59]  \n",
       "0            [46, 47, 52, 59, 66, 71, 72, 77, 78, 79]  \n",
       "1   [134, 136, 139, 112, 114, 115, 119, 123, 125, ...  \n",
       "2   [179, 181, 198, 199, 208, 209, 207, 205, 212, ...  \n",
       "\n",
       "[171 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = io_helpers.get_documents()\n",
    "filtered_docs = documents[documents[\"original_doc_ids\"].apply(func=(lambda x: True if len(x) > 0 else False))]\n",
    "\n",
    "filtered_docs\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44ae0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_compare = {\n",
    "    \"with_conflicts\": {\n",
    "        \"single\": [(134, 100134), (46, 100046), (179, 100179)],\n",
    "        \"multi\": [(139, 400139), (205, 400205), (42, 400042)],\n",
    "        \"tabular\": [(47, 300001), (136, 300002), (181, 300003)],\n",
    "    },  # 3  # 3  # 3\n",
    "    \"without_conflicts\": {\n",
    "        \"single\": [(180, 181), (56, 58), (130, 138)],\n",
    "        \"multi\": [(400204, 400075), (400192, 400194)],\n",
    "        \"tabular\": [(300003, 66), (300003, 400192), (300001, 300002)],\n",
    "    },  # 3  # 3  # 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c58ea81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 47 & 300001 -- calling openai\n",
      "Done\n",
      "-- 47 & 300001 -- calling openai\n",
      "Done\n",
      "-- 136 & 300002 -- calling openai\n",
      "Done\n",
      "-- 136 & 300002 -- calling openai\n",
      "Done\n",
      "-- 181 & 300003 -- calling openai\n",
      "Done\n",
      "-- 181 & 300003 -- calling openai\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(llm)\n",
    "\n",
    "documents = io_helpers.get_documents(read_relations=True)\n",
    "prompts = io_helpers.get_prompt(name=\"comparison/extract_contradictions\")\n",
    "\n",
    "# DOC_IDS_TO_CHECK = [300001, 100134, 134, 400192, 78]\n",
    "\n",
    "models = [\"gpt-4.1\", \"gpt-4o\"]\n",
    "for id1, id2 in docs_to_compare[\"with_conflicts\"][\"tabular\"]:\n",
    "    for model in models:\n",
    "        compare_and_extract_llm(\n",
    "            id1, id2, comparison_type=\"with_multi\", model=model, prompts=prompts, documents=documents\n",
    "        )\n",
    "\n",
    "# model = \"gpt-4.1\"\n",
    "\n",
    "# text1 = documents[documents[\"doc_id\"].astype(int) == int(doc_id1)].squeeze()[\"content\"]\n",
    "# text2 = documents[documents[\"doc_id\"].astype(int) == int(doc_id2)].squeeze()[\"content\"]\n",
    "\n",
    "# system_prompt = prompts[\"system_prompt\"]\n",
    "# user_prompt = llm.format_user_prompt(prompts[\"user_prompt\"], text1=text1, text2=text2)\n",
    "\n",
    "# response = llm.call_any_llm(\n",
    "#     system_prompt, user_prompt, model, response_format_pydantic=llm.LLMDocumentComparisonExtractResponse\n",
    "# )\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7d7027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Contradiction(quote_from_document1='L. Rogers, a 60-year-old Caucasian woman, was admitted to Knoxville General Hospital on January 6th.', quote_from_document2='Age: 56'), Contradiction(quote_from_document1='The patient, who works as a nurse in a local clinic and is married with two healthy children, has a generally healthy medical history.', quote_from_document2='Occupation: Teacher\\n...\\nOccupation and Working Conditions: Teacher at a local high school, generally well-lit and comfortable working conditions'), Contradiction(quote_from_document1='Upon examination, Dr. Nelson confirmed a preliminary diagnosis of glaucoma, which was initially suspected two months prior by an optometrist.', quote_from_document2='Diagnosis and Treatment History: Visited optometrist 2 months ago, diagnosed with early-stage cataract. Prescribed new glasses with minimal relief; no other treatments received prior to hospital admission.'), Contradiction(quote_from_document1='This was further validated by a slit-lamp examination that ruled out other ocular pathologies such as glaucoma or macular degeneration.', quote_from_document2='Differential Diagnosis:\\nGlaucoma ruled out due to normal intraocular pressure and absence of optic nerve damage. Age-related macular degeneration ruled out due to normal retinal appearance.')]\n",
      "text1: L. Rogers, a 60-year-old Caucasian woman, was admitted to Knoxville General Hospital on January 6th.\n",
      "text2: Age: 56\n",
      "-------\n",
      "text1: The patient, who works as a nurse in a local clinic and is married with two healthy children, has a generally healthy medical history.\n",
      "text2: Occupation: Teacher\n",
      "...\n",
      "Occupation and Working Conditions: Teacher at a local high school, generally well-lit and comfortable working conditions\n",
      "-------\n",
      "text1: Upon examination, Dr. Nelson confirmed a preliminary diagnosis of glaucoma, which was initially suspected two months prior by an optometrist.\n",
      "text2: Diagnosis and Treatment History: Visited optometrist 2 months ago, diagnosed with early-stage cataract. Prescribed new glasses with minimal relief; no other treatments received prior to hospital admission.\n",
      "-------\n",
      "text1: This was further validated by a slit-lamp examination that ruled out other ocular pathologies such as glaucoma or macular degeneration.\n",
      "text2: Differential Diagnosis:\n",
      "Glaucoma ruled out due to normal intraocular pressure and absence of optic nerve damage. Age-related macular degeneration ruled out due to normal retinal appearance.\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "print(response.contradictions)\n",
    "for contradiction in response.contradictions:\n",
    "    print(f\"text1: {contradiction.quote_from_document1}\")\n",
    "    print(f\"text2: {contradiction.quote_from_document2}\")\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5f5fcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.947674</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>0.936047</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-2.5-flash-preview-05-20</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  accuracy  count\n",
       "0                          gpt-4o  0.947674  172.0\n",
       "1                    gpt-4.1-mini  0.936047  172.0\n",
       "2  gemini-2.5-flash-preview-05-20  0.843023  172.0\n",
       "3                    gpt-4.1-nano  0.666667   39.0\n",
       "4                     gpt-4o-mini  0.633333   60.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### evaluate models\n",
    "from utils import evaluation\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(io_helpers)\n",
    "\n",
    "documents = io_helpers.get_documents()\n",
    "\n",
    "evaluation.evaluate_llm_comparison(documents, FILEPATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
