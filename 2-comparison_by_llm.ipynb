{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39eaac34",
   "metadata": {},
   "source": [
    "## Compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec48a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from utils import io_helpers\n",
    "from utils import llm\n",
    "\n",
    "importlib.reload(io_helpers)\n",
    "importlib.reload(llm)\n",
    "\n",
    "FILEPATH = \"evaluation/_pairwise_comparison_bool.csv\"\n",
    "FIELDNAMES = [\"id1\", \"id2\", \"conflicts\", \"model\", \"prompt\"]\n",
    "\n",
    "\n",
    "def processed_before() -> pd.DataFrame:\n",
    "    try:\n",
    "        known_results = pd.read_csv(\n",
    "            FILEPATH,\n",
    "            usecols=FIELDNAMES,\n",
    "            dtype={\"id1\": \"Int64\", \"id2\": \"Int64\", \"conflicts\": \"boolean\"},\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(os.path.dirname(FILEPATH), exist_ok=True)\n",
    "        with open(FILEPATH, \"x\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, FIELDNAMES)\n",
    "            writer.writeheader()\n",
    "        known_results = pd.DataFrame(columns=FIELDNAMES)\n",
    "    return known_results\n",
    "\n",
    "\n",
    "def pairs_generator(doc_id: int, documents: pd.DataFrame):\n",
    "    doc = documents[documents[\"doc_id\"].astype(int) == int(doc_id)].squeeze()\n",
    "    related_docs = documents[documents[\"doc_id\"].isin(doc[\"related_docs\"])]\n",
    "\n",
    "    for _, related_doc in related_docs.iterrows():\n",
    "        id1 = doc[\"doc_id\"]\n",
    "        id2 = related_doc[\"doc_id\"]\n",
    "        expected_result = (\n",
    "            (id2 in doc[\"original_doc_ids\"])\n",
    "            | (id1 in related_doc[\"original_doc_ids\"])\n",
    "            | (\n",
    "                len(set(doc[\"original_doc_ids\"]).intersection(set(related_doc[\"original_doc_ids\"]))) > 0\n",
    "            )  # share the same original_doc\n",
    "        )\n",
    "        yield (id1, id2, expected_result)\n",
    "\n",
    "\n",
    "def compare_with_llm(\n",
    "    doc_id1: int,\n",
    "    doc_id2: int,\n",
    "    expected_result: bool,\n",
    "    model: str,\n",
    "    documents: pd.DataFrame,\n",
    "    prompts: object,\n",
    "    print_results: bool = True,\n",
    "):\n",
    "    known_results = processed_before()\n",
    "    if not known_results[\n",
    "        (\n",
    "            (known_results[\"id1\"] == doc_id1) & (known_results[\"id2\"] == doc_id2)\n",
    "            | ((known_results[\"id1\"] == doc_id2) & (known_results[\"id2\"] == doc_id1))\n",
    "        )\n",
    "        & (known_results[\"model\"] == model)\n",
    "    ].empty:\n",
    "        print(f\"-- {doc_id1} & {doc_id2} -- processed before\")\n",
    "        return\n",
    "\n",
    "    system_prompt = prompts[\"system_prompt\"]\n",
    "\n",
    "    text1 = documents[documents[\"doc_id\"].astype(int) == int(doc_id1)].squeeze()[\"content\"]\n",
    "    text2 = documents[documents[\"doc_id\"].astype(int) == int(doc_id2)].squeeze()[\"content\"]\n",
    "\n",
    "    user_prompt = llm.format_user_prompt(prompts[\"user_prompt\"], text1=text1, text2=text2)\n",
    "\n",
    "    llm_response = llm.call_any_llm(\n",
    "        system_prompt, user_prompt, model, response_format_pydantic=llm.LLMDocumentComparisonCheckResponse\n",
    "    )\n",
    "    actual_result = llm_response.contradictory_info_found\n",
    "\n",
    "    if print_results:\n",
    "        print(f\"-- {doc_id1} & {doc_id2} -- \", end=\"\")\n",
    "        if expected_result == actual_result:\n",
    "            print(colored(\"Check!\", \"green\"))\n",
    "        else:\n",
    "            print(colored(\"Wrong!\", \"red\"))\n",
    "            print(f\"expected result: {expected_result}\")\n",
    "            print(f\"actual result: {actual_result}\")\n",
    "\n",
    "    new_row = {\n",
    "        \"id1\": doc_id1,\n",
    "        \"id2\": doc_id2,\n",
    "        \"conflicts\": actual_result,\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompts[\"user_prompt\"],\n",
    "    }\n",
    "    known_results = pd.concat([known_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    known_results.to_csv(FILEPATH, columns=FIELDNAMES, mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c1982934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 100134 & 134 -- processed before\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.947674</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>0.936047</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini-2.5-flash-preview-05-20</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  accuracy  count\n",
       "0                         gpt-4.1  0.986111  144.0\n",
       "1                          gpt-4o  0.947674  172.0\n",
       "2                    gpt-4.1-mini  0.936047  172.0\n",
       "3  gemini-2.5-flash-preview-05-20  0.843023  172.0\n",
       "4                    gpt-4.1-nano  0.666667   39.0\n",
       "5                     gpt-4o-mini  0.633333   60.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### evaluate models\n",
    "from utils import evaluation\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(io_helpers)\n",
    "\n",
    "FILEPATH = \"evaluation/_pairwise_comparison_bool.csv\"\n",
    "\n",
    "documents = io_helpers.get_documents(read_relations=True)\n",
    "prompts = io_helpers.get_prompt(name=\"comparison/check_for_contradictions\")\n",
    "\n",
    "# DOC_IDS_TO_CHECK = [300001, 100134, 134, 400192, 78]\n",
    "\n",
    "model = \"gpt-4.1\"\n",
    "\n",
    "for id1, id2, gt_result in pairs_generator(100134, documents):\n",
    "    compare_with_llm(id1, id2, gt_result, model=model, documents=documents, prompts=prompts, print_results=False)\n",
    "\n",
    "evaluation.evaluate_llm_comparison(documents, FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be17b5",
   "metadata": {},
   "source": [
    "## Compare and extract information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254841f6",
   "metadata": {},
   "source": [
    "### Evaluate model capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from utils import io_helpers\n",
    "from utils import llm\n",
    "import ast\n",
    "\n",
    "importlib.reload(io_helpers)\n",
    "importlib.reload(llm)\n",
    "\n",
    "FILEPATH = \"evaluation/_pairwise_comparison.csv\"\n",
    "FIELDNAMES = [\"model\", \"comparison_type\", \"id1\", \"id2\", \"contain_conflicts\", \"conflicting_passages\"]\n",
    "\n",
    "\n",
    "def processed_before() -> pd.DataFrame:\n",
    "    try:\n",
    "        known_results = pd.read_csv(\n",
    "            FILEPATH,\n",
    "            usecols=FIELDNAMES,\n",
    "            dtype={\"id1\": \"Int64\", \"id2\": \"Int64\", \"contain_conflicts\": \"boolean\"},\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(os.path.dirname(FILEPATH), exist_ok=True)\n",
    "        with open(FILEPATH, \"x\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, FIELDNAMES)\n",
    "            writer.writeheader()\n",
    "        known_results = pd.DataFrame(columns=FIELDNAMES)\n",
    "    return known_results\n",
    "\n",
    "\n",
    "def compare_and_extract_llm(\n",
    "    doc_id1: int, doc_id2: int, model: str, comparison_type: str, documents: pd.DataFrame, prompts: object\n",
    "):\n",
    "    known_results = processed_before()\n",
    "    if not known_results[\n",
    "        (\n",
    "            ((known_results[\"id1\"] == doc_id1) & (known_results[\"id2\"] == doc_id2))\n",
    "            | ((known_results[\"id1\"] == doc_id2) & (known_results[\"id2\"] == doc_id1))\n",
    "        )\n",
    "        & (known_results[\"model\"] == model)\n",
    "    ].empty:\n",
    "        print(f\"-- {doc_id1} & {doc_id2} -- processed before\")\n",
    "        return\n",
    "\n",
    "    system_prompt = prompts[\"system_prompt\"]\n",
    "\n",
    "    text1 = documents[documents[\"doc_id\"].astype(int) == int(doc_id1)].squeeze()[\"content\"]\n",
    "    text2 = documents[documents[\"doc_id\"].astype(int) == int(doc_id2)].squeeze()[\"content\"]\n",
    "\n",
    "    user_prompt = llm.format_user_prompt(prompts[\"user_prompt\"], text1=text1, text2=text2)\n",
    "\n",
    "    print(f\"-- {doc_id1} & {doc_id2} -- \", end=\"\")\n",
    "    llm_response: llm.LLMDocumentComparisonExtractResponse = llm.call_any_llm(\n",
    "        system_prompt, user_prompt, model, response_format_pydantic=llm.LLMDocumentComparisonExtractResponse\n",
    "    )\n",
    "    print(\"Done\")\n",
    "\n",
    "    new_row = {\n",
    "        \"model\": model,\n",
    "        \"comparison_type\": comparison_type,\n",
    "        \"id1\": doc_id1,\n",
    "        \"id2\": doc_id2,\n",
    "        \"contain_conflicts\": llm_response.contradictory_info_found,\n",
    "        \"conflicting_passages\": llm_response.contradictions,\n",
    "    }\n",
    "    known_results = pd.concat([known_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    known_results.to_csv(FILEPATH, columns=FIELDNAMES, mode=\"w\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41fe9754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>content</th>\n",
       "      <th>original_doc_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Acme Government Solutions is a government indu...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Entertainment Enterprises Inc. is an entertain...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Advanced Manufacturing Solutions Inc., establi...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>Finance</td>\n",
       "      <td>EcoGuard Solutions, established on April 15, 2...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Green Fields Agriculture Ltd., established on ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>400116</td>\n",
       "      <td>Law</td>\n",
       "      <td>In a significant legal proceeding at the Cedar...</td>\n",
       "      <td>[116]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>400059</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Retail Emporium, a well-established retail gia...</td>\n",
       "      <td>[59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300001</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Changes that occurred in senior management of ...</td>\n",
       "      <td>[46, 47, 52, 59, 66, 71, 72, 77, 78, 79]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300002</td>\n",
       "      <td>Law</td>\n",
       "      <td>Chief judge according to the court judgment of...</td>\n",
       "      <td>[134, 136, 139, 112, 114, 115, 119, 123, 125, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300003</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Past disease history of Y. Evans according to ...</td>\n",
       "      <td>[179, 181, 198, 199, 208, 209, 207, 205, 212, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id   domain                                            content  \\\n",
       "0       40  Finance  Acme Government Solutions is a government indu...   \n",
       "1       41  Finance  Entertainment Enterprises Inc. is an entertain...   \n",
       "2       42  Finance  Advanced Manufacturing Solutions Inc., establi...   \n",
       "3       43  Finance  EcoGuard Solutions, established on April 15, 2...   \n",
       "4       44  Finance  Green Fields Agriculture Ltd., established on ...   \n",
       "..     ...      ...                                                ...   \n",
       "28  400116      Law  In a significant legal proceeding at the Cedar...   \n",
       "29  400059  Finance  Retail Emporium, a well-established retail gia...   \n",
       "0   300001  Finance  Changes that occurred in senior management of ...   \n",
       "1   300002      Law  Chief judge according to the court judgment of...   \n",
       "2   300003  Medical  Past disease history of Y. Evans according to ...   \n",
       "\n",
       "                                     original_doc_ids  \n",
       "0                                                  []  \n",
       "1                                                  []  \n",
       "2                                                  []  \n",
       "3                                                  []  \n",
       "4                                                  []  \n",
       "..                                                ...  \n",
       "28                                              [116]  \n",
       "29                                               [59]  \n",
       "0            [46, 47, 52, 59, 66, 71, 72, 77, 78, 79]  \n",
       "1   [134, 136, 139, 112, 114, 115, 119, 123, 125, ...  \n",
       "2   [179, 181, 198, 199, 208, 209, 207, 205, 212, ...  \n",
       "\n",
       "[171 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = io_helpers.get_documents()\n",
    "filtered_docs = documents[documents[\"original_doc_ids\"].apply(func=(lambda x: True if len(x) > 0 else False))]\n",
    "\n",
    "filtered_docs\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44ae0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_compare = {\n",
    "    \"with_conflicts\": {\n",
    "        \"single\": [(134, 100134), (46, 100046), (179, 100179)],\n",
    "        \"multi\": [(139, 400139), (205, 400205), (42, 400042)],\n",
    "        \"tabular\": [(47, 300001), (136, 300002), (181, 300003)],\n",
    "    },  # 3  # 3  # 3\n",
    "    \"without_conflicts\": {\n",
    "        \"single\": [(180, 181), (56, 58), (130, 138)],\n",
    "        \"multi\": [(400204, 400075), (400192, 400194)],\n",
    "        \"tabular\": [(300003, 66), (300003, 400192), (300001, 300002)],\n",
    "    },  # 3  # 3  # 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c58ea81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 47 & 300001 -- calling openai\n",
      "Done\n",
      "-- 47 & 300001 -- calling openai\n",
      "Done\n",
      "-- 136 & 300002 -- calling openai\n",
      "Done\n",
      "-- 136 & 300002 -- calling openai\n",
      "Done\n",
      "-- 181 & 300003 -- calling openai\n",
      "Done\n",
      "-- 181 & 300003 -- calling openai\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(llm)\n",
    "\n",
    "documents = io_helpers.get_documents(read_relations=True)\n",
    "prompts = io_helpers.get_prompt(name=\"comparison/extract_contradictions\")\n",
    "\n",
    "# DOC_IDS_TO_CHECK = [300001, 100134, 134, 400192, 78]\n",
    "\n",
    "models = [\"gpt-4.1\", \"gpt-4o\"]\n",
    "for id1, id2 in docs_to_compare[\"with_conflicts\"][\"tabular\"]:\n",
    "    for model in models:\n",
    "        compare_and_extract_llm(\n",
    "            id1, id2, comparison_type=\"with_multi\", model=model, prompts=prompts, documents=documents\n",
    "        )\n",
    "\n",
    "# model = \"gpt-4.1\"\n",
    "\n",
    "# text1 = documents[documents[\"doc_id\"].astype(int) == int(doc_id1)].squeeze()[\"content\"]\n",
    "# text2 = documents[documents[\"doc_id\"].astype(int) == int(doc_id2)].squeeze()[\"content\"]\n",
    "\n",
    "# system_prompt = prompts[\"system_prompt\"]\n",
    "# user_prompt = llm.format_user_prompt(prompts[\"user_prompt\"], text1=text1, text2=text2)\n",
    "\n",
    "# response = llm.call_any_llm(\n",
    "#     system_prompt, user_prompt, model, response_format_pydantic=llm.LLMDocumentComparisonExtractResponse\n",
    "# )\n",
    "\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4f67f",
   "metadata": {},
   "source": [
    "### Extract and save all contradictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b05b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils import io_helpers\n",
    "from utils import llm\n",
    "\n",
    "importlib.reload(io_helpers)\n",
    "importlib.reload(llm)\n",
    "\n",
    "FIELDNAMES = [\"doc_id1\", \"doc_id2\", \"conflicting_passage_doc1\", \"conflicting_passage_doc2\", \"model\"]\n",
    "\n",
    "\n",
    "def get_processed_before(filepath: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        processed_pairs = pd.read_csv(\n",
    "            filepath,\n",
    "            usecols=[\"doc_id1\", \"doc_id2\"],\n",
    "            dtype={\"doc_id1\": \"Int64\", \"doc_id2\": \"Int64\"},\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        with open(filepath, \"x\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, [\"doc_id1\", \"doc_id2\"])\n",
    "            writer.writeheader()\n",
    "        processed_pairs = pd.DataFrame(columns=[\"doc_id1\", \"doc_id2\"])\n",
    "    return processed_pairs\n",
    "\n",
    "\n",
    "def set_processed_before(filepath: str, data: pd.DataFrame):\n",
    "    data.to_csv(filepath, mode=\"w\", columns=[\"doc_id1\", \"doc_id2\"], index=False)\n",
    "\n",
    "\n",
    "def save_result(filepath: str, data: pd.DataFrame):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        pd.concat([df, data], ignore_index=True).to_csv(filepath, mode=\"w\", index=False)\n",
    "    except FileNotFoundError:\n",
    "        data.to_csv(filepath, mode=\"w\", index=False)\n",
    "\n",
    "\n",
    "def extract_conflicts(\n",
    "    doc_id1: int,\n",
    "    doc_id2: int,\n",
    "    documents: pd.DataFrame,\n",
    "    prompts: object,\n",
    "    filepath_results=str,\n",
    "    filepath_processed_before=str,\n",
    "):\n",
    "    model = \"gpt-4.1\"\n",
    "    processed_pairs: pd.DataFrame = get_processed_before(filepath_processed_before)\n",
    "    if not processed_pairs[\n",
    "        ((processed_pairs[\"doc_id1\"] == doc_id1) & (processed_pairs[\"doc_id2\"] == doc_id2))\n",
    "        | ((processed_pairs[\"doc_id1\"] == doc_id2) & (processed_pairs[\"doc_id2\"] == doc_id1))\n",
    "    ].empty:\n",
    "        print(f\"-- {doc_id1} & {doc_id2} -- processed before\")\n",
    "        return\n",
    "\n",
    "    system_prompt = prompts[\"system_prompt\"]\n",
    "\n",
    "    text1 = documents[documents[\"doc_id\"].astype(int) == int(doc_id1)].squeeze()[\"content\"]\n",
    "    text2 = documents[documents[\"doc_id\"].astype(int) == int(doc_id2)].squeeze()[\"content\"]\n",
    "\n",
    "    user_prompt = llm.format_user_prompt(prompts[\"user_prompt\"], text1=text1, text2=text2)\n",
    "\n",
    "    print(f\"-- {doc_id1} & {doc_id2} --\", end=\" \")\n",
    "    llm_response: llm.LLMDocumentComparisonExtractResponse = llm.call_any_llm(\n",
    "        system_prompt, user_prompt, model, response_format_pydantic=llm.LLMDocumentComparisonExtractResponse\n",
    "    )\n",
    "    if len(llm_response.contradictions) == 0:\n",
    "        print(\"No\", end=\" \")\n",
    "    else:\n",
    "        print(len(llm_response.contradictions), end=\" \")\n",
    "    print(\"contradictions found.\")\n",
    "\n",
    "    for conflict in llm_response.contradictions:\n",
    "        new_result = {\n",
    "            \"id1\": doc_id1,\n",
    "            \"id2\": doc_id2,\n",
    "            \"model\": model,\n",
    "            \"conflicting_passage_doc1\": conflict.quote_from_document1,\n",
    "            \"conflicting_passage_doc2\": conflict.quote_from_document2,\n",
    "        }\n",
    "        save_result(filepath_results, pd.DataFrame([new_result]))\n",
    "\n",
    "    processed_pairs = pd.concat(\n",
    "        [processed_pairs, pd.DataFrame([{\"doc_id1\": doc_id1, \"doc_id2\": doc_id2}])], ignore_index=True\n",
    "    )\n",
    "    set_processed_before(filepath_processed_before, processed_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "391eb0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100134, 134),\n",
       " (100134, 300002),\n",
       " (100136, 136),\n",
       " (100136, 300002),\n",
       " (100139, 139)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get all documents pairs that really contain conflicts\n",
    "\n",
    "documents = io_helpers.get_documents()\n",
    "pairs = []\n",
    "\n",
    "\n",
    "def get_conflicting_docs(row):\n",
    "    if len(row[\"original_doc_ids\"]) == 0:\n",
    "        return\n",
    "    else:\n",
    "        for original_doc_id in row[\"original_doc_ids\"]:\n",
    "            pairs.append((int(row[\"doc_id\"]), int(original_doc_id)))\n",
    "\n",
    "        if str(row[\"doc_id\"]).startswith(\"100\"):\n",
    "            (\n",
    "                pairs.append((row[\"doc_id\"], 300001))\n",
    "                if row[\"original_doc_ids\"][0]\n",
    "                in documents.loc[documents[\"doc_id\"] == 300001].squeeze()[\"original_doc_ids\"]\n",
    "                else None\n",
    "            )\n",
    "            (\n",
    "                pairs.append((row[\"doc_id\"], 300002))\n",
    "                if row[\"original_doc_ids\"][0]\n",
    "                in documents.loc[documents[\"doc_id\"] == 300002].squeeze()[\"original_doc_ids\"]\n",
    "                else None\n",
    "            )\n",
    "            (\n",
    "                pairs.append((row[\"doc_id\"], 300003))\n",
    "                if row[\"original_doc_ids\"][0]\n",
    "                in documents.loc[documents[\"doc_id\"] == 300003].squeeze()[\"original_doc_ids\"]\n",
    "                else None\n",
    "            )\n",
    "\n",
    "\n",
    "documents.apply(get_conflicting_docs, axis=1)\n",
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50770f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 100134 & 134 -- processed before\n",
      "-- 100134 & 300002 -- 1 contradictions found.\n",
      "-- 100136 & 136 -- 3 contradictions found.\n",
      "-- 100136 & 300002 -- 1 contradictions found.\n",
      "-- 100139 & 139 -- 1 contradictions found.\n",
      "-- 100139 & 300002 -- 1 contradictions found.\n",
      "-- 100046 & 46 -- processed before\n",
      "-- 100046 & 300001 -- No contradictions found.\n",
      "-- 100047 & 47 -- 1 contradictions found.\n",
      "-- 100047 & 300001 -- 1 contradictions found.\n",
      "-- 100179 & 179 -- processed before\n",
      "-- 100179 & 300003 -- 1 contradictions found.\n",
      "-- 100052 & 52 -- No contradictions found.\n",
      "-- 100052 & 300001 -- 1 contradictions found.\n",
      "-- 100181 & 181 -- No contradictions found.\n",
      "-- 100181 & 300003 -- 1 contradictions found.\n",
      "-- 100059 & 59 -- 1 contradictions found.\n",
      "-- 100059 & 300001 -- 1 contradictions found.\n",
      "-- 100066 & 66 -- 1 contradictions found.\n",
      "-- 100066 & 300001 -- 1 contradictions found.\n",
      "-- 100198 & 198 -- 1 contradictions found.\n",
      "-- 100198 & 300003 -- 1 contradictions found.\n",
      "-- 100071 & 71 -- 1 contradictions found.\n",
      "-- 100071 & 300001 -- 1 contradictions found.\n",
      "-- 100072 & 72 -- 1 contradictions found.\n",
      "-- 100072 & 300001 -- 1 contradictions found.\n",
      "-- 100199 & 199 -- 1 contradictions found.\n",
      "-- 100199 & 300003 -- 1 contradictions found.\n",
      "-- 100077 & 77 -- 2 contradictions found.\n",
      "-- 100077 & 300001 -- 1 contradictions found.\n",
      "-- 100078 & 78 -- 1 contradictions found.\n",
      "-- 100078 & 300001 -- 1 contradictions found.\n",
      "-- 100079 & 79 -- 1 contradictions found.\n",
      "-- 100079 & 300001 -- 1 contradictions found.\n",
      "-- 100208 & 208 -- No contradictions found.\n",
      "-- 100208 & 300003 -- 1 contradictions found.\n",
      "-- 100209 & 209 -- 1 contradictions found.\n",
      "-- 100209 & 300003 -- 1 contradictions found.\n",
      "-- 100207 & 207 -- 1 contradictions found.\n",
      "-- 100207 & 300003 -- 1 contradictions found.\n",
      "-- 100205 & 205 -- 2 contradictions found.\n",
      "-- 100205 & 300003 -- 1 contradictions found.\n",
      "-- 100212 & 212 -- 1 contradictions found.\n",
      "-- 100212 & 300003 -- 1 contradictions found.\n",
      "-- 100211 & 211 -- 1 contradictions found.\n",
      "-- 100211 & 300003 -- 1 contradictions found.\n",
      "-- 100112 & 112 -- 2 contradictions found.\n",
      "-- 100112 & 300002 -- 2 contradictions found.\n",
      "-- 100114 & 114 -- 1 contradictions found.\n",
      "-- 100114 & 300002 -- No contradictions found.\n",
      "-- 100115 & 115 -- 1 contradictions found.\n",
      "-- 100115 & 300002 -- 1 contradictions found.\n",
      "-- 100119 & 119 -- 1 contradictions found.\n",
      "-- 100119 & 300002 -- 1 contradictions found.\n",
      "-- 100123 & 123 -- 1 contradictions found.\n",
      "-- 100123 & 300002 -- 1 contradictions found.\n",
      "-- 100125 & 125 -- 21 contradictions found.\n",
      "-- 100125 & 300002 -- 2 contradictions found.\n",
      "-- 100127 & 127 -- 2 contradictions found.\n",
      "-- 100127 & 300002 -- 1 contradictions found.\n",
      "-- 400128 & 128 -- 2 contradictions found.\n",
      "-- 400132 & 132 -- 5 contradictions found.\n",
      "-- 400133 & 133 -- 2 contradictions found.\n",
      "-- 400134 & 134 -- 6 contradictions found.\n",
      "-- 400136 & 136 -- 2 contradictions found.\n",
      "-- 400205 & 205 -- 6 contradictions found.\n",
      "-- 400139 & 139 -- 2 contradictions found.\n",
      "-- 400040 & 40 -- 5 contradictions found.\n",
      "-- 400042 & 42 -- 6 contradictions found.\n",
      "-- 400052 & 52 -- 10 contradictions found.\n",
      "-- 400053 & 53 -- 5 contradictions found.\n",
      "-- 400183 & 183 -- 6 contradictions found.\n",
      "-- 400188 & 188 -- 3 contradictions found.\n",
      "-- 400192 & 192 -- 4 contradictions found.\n",
      "-- 400065 & 65 -- 11 contradictions found.\n",
      "-- 400194 & 194 -- 1 contradictions found.\n",
      "-- 400198 & 198 -- 3 contradictions found.\n",
      "-- 400199 & 199 -- 2 contradictions found.\n",
      "-- 400074 & 74 -- 4 contradictions found.\n",
      "-- 400202 & 202 -- 2 contradictions found.\n",
      "-- 400204 & 204 -- 6 contradictions found.\n",
      "-- 400075 & 75 -- 9 contradictions found.\n",
      "-- 400078 & 78 -- 6 contradictions found.\n",
      "-- 400079 & 79 -- 7 contradictions found.\n",
      "-- 400207 & 207 -- 3 contradictions found.\n",
      "-- 400213 & 213 -- 6 contradictions found.\n",
      "-- 400214 & 214 -- 1 contradictions found.\n",
      "-- 400110 & 110 -- 3 contradictions found.\n",
      "-- 400116 & 116 -- 3 contradictions found.\n",
      "-- 400059 & 59 -- 6 contradictions found.\n",
      "-- 300001 & 46 -- 1 contradictions found.\n",
      "-- 300001 & 47 -- 1 contradictions found.\n",
      "-- 300001 & 52 -- 2 contradictions found.\n",
      "-- 300001 & 59 -- 1 contradictions found.\n",
      "-- 300001 & 66 -- 1 contradictions found.\n",
      "-- 300001 & 71 -- 2 contradictions found.\n",
      "-- 300001 & 72 -- 1 contradictions found.\n",
      "-- 300001 & 77 -- 1 contradictions found.\n",
      "-- 300001 & 78 -- 2 contradictions found.\n",
      "-- 300001 & 79 -- 2 contradictions found.\n",
      "-- 300002 & 134 -- 2 contradictions found.\n",
      "-- 300002 & 136 -- 6 contradictions found.\n",
      "-- 300002 & 139 -- 2 contradictions found.\n",
      "-- 300002 & 112 -- 4 contradictions found.\n",
      "-- 300002 & 114 -- 2 contradictions found.\n",
      "-- 300002 & 115 -- 4 contradictions found.\n",
      "-- 300002 & 119 -- 2 contradictions found.\n",
      "-- 300002 & 123 -- 2 contradictions found.\n",
      "-- 300002 & 125 -- 3 contradictions found.\n",
      "-- 300002 & 127 -- 1 contradictions found.\n",
      "-- 300003 & 179 -- 4 contradictions found.\n",
      "-- 300003 & 181 -- 4 contradictions found.\n",
      "-- 300003 & 198 -- 5 contradictions found.\n",
      "-- 300003 & 199 -- 5 contradictions found.\n",
      "-- 300003 & 208 -- 3 contradictions found.\n",
      "-- 300003 & 209 -- 2 contradictions found.\n",
      "-- 300003 & 207 -- 2 contradictions found.\n",
      "-- 300003 & 205 -- 3 contradictions found.\n",
      "-- 300003 & 212 -- 3 contradictions found.\n",
      "-- 300003 & 211 -- 2 contradictions found.\n"
     ]
    }
   ],
   "source": [
    "### Extract and save conflicting passages\n",
    "\n",
    "importlib.reload(llm)\n",
    "\n",
    "documents = io_helpers.get_documents(read_relations=True)\n",
    "prompts = io_helpers.get_prompt(name=\"comparison/extract_contradictions\")\n",
    "\n",
    "# docs_to_compare = [(134, 100134), (46, 100046), (179, 100179)]\n",
    "\n",
    "for id1, id2 in pairs:\n",
    "    extract_conflicts(\n",
    "        id1,\n",
    "        id2,\n",
    "        prompts=prompts,\n",
    "        documents=documents,\n",
    "        filepath_processed_before=\"utils/llm_tracking/_conflicts_evaluated.csv\",\n",
    "        filepath_results=\"data/additional_data/docs/_conflicts.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f5fcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.947674</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4.1-mini</td>\n",
       "      <td>0.936047</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini-2.5-flash-preview-05-20</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  accuracy  count\n",
       "0                         gpt-4.1  0.986111  144.0\n",
       "1                          gpt-4o  0.947674  172.0\n",
       "2                    gpt-4.1-mini  0.936047  172.0\n",
       "3  gemini-2.5-flash-preview-05-20  0.843023  172.0\n",
       "4                    gpt-4.1-nano  0.666667   39.0\n",
       "5                     gpt-4o-mini  0.633333   60.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### evaluate models\n",
    "from utils import evaluation\n",
    "\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(io_helpers)\n",
    "\n",
    "FILEPATH = \"evaluation/_pairwise_comparison_bool.csv\"\n",
    "\n",
    "documents = io_helpers.get_documents()\n",
    "\n",
    "evaluation.evaluate_llm_comparison(documents, FILEPATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
