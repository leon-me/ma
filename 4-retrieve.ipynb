{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7815f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "import importlib\n",
    "from langchain.vectorstores import LanceDB\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# DB specifications\n",
    "LANCEDB_DIR = \"/Users/leon/Documents/study/MA/lancedb\"\n",
    "TABLE_NAME_DOCS = \"documents\"\n",
    "TABLE_NAME_CHUNKS = \"chunks_emb-large\"\n",
    "\n",
    "db = lancedb.connect(LANCEDB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b6f8e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = LanceDB(\n",
    "    uri=LANCEDB_DIR,\n",
    "    embedding=embeddings,\n",
    "    table=db.open_table(TABLE_NAME_CHUNKS),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "244b0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def retrieve_newer_chunks(retrieved_chunks) -> List:\n",
    "    while True:\n",
    "        outdated_by_ids = {\n",
    "            id_ for chunk in retrieved_chunks for id_ in chunk.metadata.get(\"outdated_by_chunk_ids\", [])\n",
    "        }\n",
    "        chunk_ids = {chunk.metadata[\"chunk_id\"] for chunk in retrieved_chunks}\n",
    "        outdated_by_ids -= chunk_ids\n",
    "\n",
    "        if len(outdated_by_ids) == 0:\n",
    "            break\n",
    "\n",
    "        table = db.open_table(TABLE_NAME_CHUNKS)\n",
    "        if len(outdated_by_ids) == 1:\n",
    "            table_data = table.search().where(f\"id == {next(iter(outdated_by_ids))}\").to_pandas()\n",
    "        else:\n",
    "            table_data = table.search().where(f\"id IN {tuple(outdated_by_ids)}\").to_pandas()\n",
    "\n",
    "        # Flatten metadata column into top-level columns\n",
    "        metadata_df = table_data[\"metadata\"].apply(pd.Series)\n",
    "\n",
    "        # Drop the original nested metadata column\n",
    "        table_data = table_data.drop(columns=[\"metadata\"])\n",
    "\n",
    "        # Merge the expanded metadata back\n",
    "        flattened_df = pd.concat([table_data, metadata_df], axis=1)\n",
    "        loader = DataFrameLoader(flattened_df, page_content_column=\"text\")\n",
    "        additional_chunks = loader.load()\n",
    "        retrieved_chunks.extend(additional_chunks)\n",
    "\n",
    "    return retrieved_chunks\n",
    "\n",
    "\n",
    "def filter_outdated_chunks(chunks: List) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    - (chunks_current, chunks_outdated)\n",
    "    \"\"\"\n",
    "    chunks_outdated = []\n",
    "    chunks_current = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk.metadata[\"outdated_by_chunk_ids\"]) != 0:\n",
    "            chunks_outdated.append(chunk)\n",
    "        else:\n",
    "            chunks_current.append(chunk)\n",
    "    return (chunks_current, chunks_outdated)\n",
    "\n",
    "\n",
    "def sort_chunks(chunks: List) -> List:\n",
    "    chunks.sort(reverse=True, key=lambda c: c.metadata[\"creation_date\"])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "74c8deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, k=5):\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=k)\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "89c63d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from utils import llm\n",
    "\n",
    "importlib.reload(llm)\n",
    "\n",
    "\n",
    "def generate(query: str, context: List, use_dq_assessment=True, model: str = \"gpt-4.1\"):\n",
    "    docs_content = \"\\n\\n-- new chunk --\\n\".join(f\"{doc.page_content}\" for doc in context)\n",
    "\n",
    "    system_prompt = \"You are an expert in question answering from a given context.\"\n",
    "    if use_dq_assessment:\n",
    "        user_prompt = f'Answer the given question as short as possible based on the given context. The context consist of multiple chunks which are devided by \"-- new chunk --\". The chunks are ordered based on their currency with the most current one at the beginning. If they include multiple conflictings answers to the question, take the one that appears first in the context.\\nContext:\\n{docs_content}\\n\\nQuestion: {query}'\n",
    "    else:\n",
    "        user_prompt = f\"Answer the given question as short as possible based on the context, which has been retrieved by similarity search from a knowledge base.\\nContext:\\n{docs_content}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    return llm.call_openai(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        model,\n",
    "        response_format_pydantic=llm.LLMQAResponse,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48748e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[\u001b[0m2025-06-15T18:23:17Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:17Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:18Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:19Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:20Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:21Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:23Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:27Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:28Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:29Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:32Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:34Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:36Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:37Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:40Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:40Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:41Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:41Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:42Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:43Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:43Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:44Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:45Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:45Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:47Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:49Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:51Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:52Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:52Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:54Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:54Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:55Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:55Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:56Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n",
      "\u001b[90m[\u001b[0m2025-06-15T18:23:58Z \u001b[33mWARN \u001b[0m lance::dataset::scanner\u001b[90m]\u001b[0m nprobes is not set because nearest has not been called yet\n"
     ]
    }
   ],
   "source": [
    "### Evaluate retrieval\n",
    "from utils import io_helpers\n",
    "import pandas as pd\n",
    "from langchain_core.load import dumps, loads, load, serializable\n",
    "\n",
    "importlib.reload(io_helpers)\n",
    "\n",
    "queries_manipulated = io_helpers.get_queries(\"manipulated_only\")\n",
    "\n",
    "\n",
    "def retrieve_context(query_entry: pd.Series, top_k: int, with_dq: bool = True):\n",
    "    \"\"\"Retrieves context from lancedb and saves it to csv\"\"\"\n",
    "    filename = (\n",
    "        f\"evaluation/retrieval_results/_query_with_context_with_DQ_{top_k}.csv\"\n",
    "        if with_dq\n",
    "        else f\"evaluation/retrieval_results/_query_with_context_without_DQ_{top_k}.csv\"\n",
    "    )\n",
    "\n",
    "    chunks = retrieve(query_entry[\"query.content\"], k=top_k)\n",
    "    if with_dq:\n",
    "        chunks = retrieve_newer_chunks(chunks)\n",
    "        chunks = sort_chunks(chunks)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata.pop(\"creation_date\", None)\n",
    "        chunk.metadata.pop(\"vector\", None)\n",
    "        chunk.metadata[\"outdated_by_chunk_ids\"] = list(chunk.metadata[\"outdated_by_chunk_ids\"])  # np.array not allowed\n",
    "\n",
    "    chunks_s = [dumps(chunk) for chunk in chunks]\n",
    "\n",
    "    query_entry[\"context\"] = chunks_s\n",
    "    try:\n",
    "        data = pd.read_csv(filename)\n",
    "        data = pd.concat([data, pd.DataFrame([query_entry])], sort=False)\n",
    "    except FileNotFoundError:\n",
    "        data = pd.DataFrame([query_entry])\n",
    "    data.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "query = queries_manipulated.sample(n=100, random_state=1)\n",
    "\n",
    "_ = query.apply(retrieve_context, args=(3, False), axis=1)\n",
    "_ = query.apply(retrieve_context, args=(3, True), axis=1)\n",
    "# _ = query.apply(retrieve_context, args=(5, False), axis=1)\n",
    "# _ = query.apply(retrieve_context, args=(5, True), axis=1)\n",
    "# _ = query.apply(retrieve_context, args=(7, False), axis=1)\n",
    "# _ = query.apply(retrieve_context, args=(7, True), axis=1)\n",
    "# _ = query.apply(retrieve_context, args=(10, False), axis=1)\n",
    "# _ = query.apply(retrieve_context, args=(10, True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9ac0986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top_k: 3\n",
      "small: 63.00 %\tlarge: 63.00 %\t(with DQ)\n",
      "small: 46.00 %\tlarge: 46.00 %\t(without DQ)\n",
      "\n",
      "Top_k: 5\n",
      "small: 74.00 %\tlarge: 66.00 %\t(with DQ)\n",
      "small: 62.00 %\tlarge: 56.00 %\t(without DQ)\n",
      "\n",
      "Top_k: 7\n",
      "small: 76.00 %\tlarge: 69.00 %\t(with DQ)\n",
      "small: 65.00 %\tlarge: 60.00 %\t(without DQ)\n",
      "\n",
      "Top_k: 10\n",
      "small: 78.00 %\tlarge: 72.00 %\t(with DQ)\n",
      "small: 71.00 %\tlarge: 66.00 %\t(without DQ)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.load import loads\n",
    "import ast\n",
    "import numpy as np\n",
    "import string\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def calculate_recall(\n",
    "    top_k: int, with_dq: bool = True, print_info_if_wrong: bool = False, emb: Literal[\"_large\", \"\"] = \"\"\n",
    "):\n",
    "    filename = (\n",
    "        f\"evaluation/retrieval_results/_query_with_context_with_DQ_{top_k}{emb}.csv\"\n",
    "        if with_dq\n",
    "        else f\"evaluation/retrieval_results/_query_with_context_without_DQ_{top_k}{emb}.csv\"\n",
    "    )\n",
    "    data = pd.read_csv(\n",
    "        filename,\n",
    "        converters={\"context\": ast.literal_eval, \"ground_truth.references\": ast.literal_eval},\n",
    "    )\n",
    "    data[\"context\"] = data[\"context\"].apply(lambda chunks: [loads(c) for c in chunks])\n",
    "\n",
    "    hits = []\n",
    "    for _, row in data.iterrows():\n",
    "        complete_context_txt = \"\\n\".join([chunk.page_content for chunk in row[\"context\"]]).lower()\n",
    "        hit_in_row = True\n",
    "        for ref in row[\"ground_truth.references\"]:\n",
    "            hit_in_row = hit_in_row and (ref.lower().strip() in complete_context_txt)\n",
    "            if print_info_if_wrong and (not (ref.lower().strip().strip(string.punctuation) in complete_context_txt)):\n",
    "                print(\"------\")\n",
    "                print(f\"{row[\"query.content\"]}: {row[\"ground_truth.content\"]}, {row[\"ground_truth.doc_ids\"]}\")\n",
    "                print(\"Reference:\")\n",
    "                print(ref.lower().strip())\n",
    "                print()\n",
    "                print(\"Metadata:\")\n",
    "                print(\"\\n\".join([str(c.metadata) for c in row[\"context\"]]))\n",
    "                print(\"Context:\")\n",
    "                print(\"\\n- \".join([c.page_content for c in row[\"context\"]]))\n",
    "        hits.append(hit_in_row)\n",
    "    recall = np.mean(hits)\n",
    "    return recall\n",
    "\n",
    "\n",
    "for k in [3, 5, 7, 10]:\n",
    "    print(f\"Top_k: {k}\")\n",
    "    print(\n",
    "        f\"small: {calculate_recall(k, True)*100:.2f} %\\tlarge: {calculate_recall(k, True, emb=\"_large\")*100:.2f} %\\t(with DQ)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"small: {calculate_recall(k, False)*100:.2f} %\\tlarge: {calculate_recall(k, False, emb=\"_large\")*100:.2f} %\\t(without DQ)\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "27c7fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without DQ:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[178]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m query_content = \u001b[33m\"\u001b[39m\u001b[33mAccording to the judgment of Glenwood, Quailwood, Court, what was the occupation of Y. Nelson?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWithout DQ:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m context = \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m response = generate(query, context, use_dq_assessment=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[174]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mretrieve\u001b[39m\u001b[34m(query, k)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, k=\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     retrieved_docs = \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retrieved_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/langchain_community/vectorstores/lancedb.py:530\u001b[39m, in \u001b[36mLanceDB.similarity_search\u001b[39m\u001b[34m(self, query, k, name, filter, fts, **kwargs)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    508\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m     **kwargs: Any,\n\u001b[32m    514\u001b[39m ) -> List[Document]:\n\u001b[32m    515\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return documents most similar to the query\u001b[39;00m\n\u001b[32m    516\u001b[39m \n\u001b[32m    517\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m \u001b[33;03m        List of documents most similar to the query.\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/langchain_community/vectorstores/lancedb.py:502\u001b[39m, in \u001b[36mLanceDB.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    499\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFull text/ Hybrid search is not supported in LanceDB Cloud yet.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    500\u001b[39m         )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._query(embedding, k, \u001b[38;5;28mfilter\u001b[39m=\u001b[38;5;28mfilter\u001b[39m, **kwargs)\n\u001b[32m    504\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.results_to_docs(res, score=score)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/langchain_community/embeddings/openai.py:704\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    696\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m    697\u001b[39m \n\u001b[32m    698\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    702\u001b[39m \u001b[33;03m        Embedding for the text.\u001b[39;00m\n\u001b[32m    703\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/langchain_community/embeddings/openai.py:671\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size)\u001b[39m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    670\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/langchain_community/embeddings/openai.py:474\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.endswith(\u001b[33m\"\u001b[39m\u001b[33m001\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    469\u001b[39m     \u001b[38;5;66;03m# See: https://github.com/openai/openai-python/\u001b[39;00m\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m#      issues/418#issuecomment-1525939500\u001b[39;00m\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[32m    472\u001b[39m     text = text.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m token = \u001b[43mencoding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Split tokens into chunks respecting the embedding_ctx_length\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token), \u001b[38;5;28mself\u001b[39m.embedding_ctx_length):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ma/lib/python3.12/site-packages/tiktoken/core.py:120\u001b[39m, in \u001b[36mEncoding.encode\u001b[39m\u001b[34m(self, text, allowed_special, disallowed_special)\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[32m    119\u001b[39m         disallowed_special = \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m match := \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    121\u001b[39m         raise_disallowed_special_token(match.group())\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "query_content = \"According to the judgment of Glenwood, Quailwood, Court, what was the occupation of Y. Nelson?\"\n",
    "\n",
    "\n",
    "print(\"Without DQ:\")\n",
    "context = retrieve(query)\n",
    "response = generate(query, context, use_dq_assessment=False)\n",
    "print(response.answer)\n",
    "print()\n",
    "\n",
    "print(\"With DQ:\")\n",
    "context = retrieve(query, k=5)\n",
    "context = retrieve_newer_chunks(context)\n",
    "context = sort_chunks(context)\n",
    "response = generate(query, context=context)\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f1a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300002\n",
      "2015-01-01 00:00:00\n",
      "3705 - outdated by []\n",
      "- content:\n",
      "Chief judge according to the court judgment of Danbury, Pinehurst, Court | J. Smith\n",
      "Residence of F. Williams according to the court judgment of Upton, Georgetown, Court | 45, Maple Avenue, Georgetown.\n",
      "Defense lawyer for Y. Nelson according to the judgment of Glenwood, Quailwood, Court | J. Smith\n",
      "Date of the court judgment of Bayside, Roseville, Court | 15th November, 2022\n",
      "Name of the chief judge according to the court judgment of Trenton, Eastwood, Court | J. Smith\n",
      "-----------\n",
      "400139\n",
      "2010-01-01 00:00:00\n",
      "3544 - outdated by []\n",
      "- content:\n",
      "In a significant case adjudicated by the 9th Judicial Circuit of Glenwood, the Glenwood, Quailwood Court delivered a judgment on June 10, 2023, against a female defendant, Y. Nelson. The court proceedings were overseen by Chief Judge Hon. H. Ruiz and Presiding Judge Hon. E. Collins, with K. Kelly serving as the court clerk.\n",
      "-----------\n",
      "400139\n",
      "2010-01-01 00:00:00\n",
      "3549 - outdated by []\n",
      "- content:\n",
      "Y. Nelson was represented by defense lawyer L. Johnson from Parker & Associates during the trial. The judgment also outlined her right to appeal within ten days from the judgment date, which could be submitted through the Glenwood, Quailwood Court if pursued.\n",
      "-----------\n",
      "400139\n",
      "2010-01-01 00:00:00\n",
      "3547 - outdated by []\n",
      "- content:\n",
      "The Glenwood, Quailwood Procuratorate, spearheading the prosecution, highlighted Nelson’s aggressive behavior, particularly between February 10 and February 20, 2023, when she forcibly took goods worth approximately $800 from local vendors without payment. This action was backed by evidence recovered from her residence. Her continued misconduct culminated in an incident on March 12, 2023, when she obstructed traffic and shouted obscenities near a school, creating chaos and alarm among parents and students.\n",
      "-----------\n",
      "400139\n",
      "2010-01-01 00:00:00\n",
      "3545 - outdated by []\n",
      "- content:\n",
      "The charges stem from a series of public disturbances caused by Y. Nelson, a chef, between January 15 and March 12, 2023. Reports first emerged on February 10, 2023, leading to an extensive investigation by local authorities. It was uncovered that Nelson engaged in persistent provocative and quarrelsome activities in downtown Quailwood's public spaces, causing severe disruptions to businesses and distress among residents. The disturbances included verbal harassment in public places and obstructing traffic\n",
      "-----------\n",
      "100139\n",
      "2005-01-01 00:00:00\n",
      "2817 - outdated by []\n",
      "- content:\n",
      "Glenwood, Quailwood Court\n",
      "9th Judicial Circuit\n",
      "State of Glenwood\n",
      "\n",
      "Case No: 2023-458-CR\n",
      "\n",
      "Chief Judge: Hon. H. Ruiz\n",
      "Presiding Judge: Hon. E. Collins\n",
      "Court Clerk: K. Kelly\n",
      "\n",
      "---\n",
      "\n",
      "**JUDGMENT**\n",
      "\n",
      "**The People of Glenwood vs. Y. Nelson**\n",
      "\n",
      "**1. Court and Prosecutor Information:**\n",
      "\n",
      "*Court:* Glenwood, Quailwood Court  \n",
      "*Prosecutor:* Glenwood, Quailwood Procuratorate  \n",
      "\n",
      "*Chief Judge:* Hon. H. Ruiz  \n",
      "*Presiding Judge:* Hon. E. Collins  \n",
      "*Court Clerk:* K. Kelly  \n",
      "\n",
      "**2. Defendant and Defense Lawyer Information:**\n",
      "-----------\n",
      "100139\n",
      "2005-01-01 00:00:00\n",
      "2821 - outdated by []\n",
      "- content:\n",
      "*Arrest:*  \n",
      "On March 15, 2023, the Glenwood, Quailwood Procuratorate issued an arrest warrant for Y. Nelson upon finding sufficient evidence to substantiate the charges.\n",
      "\n",
      "**4. Case Statement:**\n",
      "\n",
      "The case revolves around a series of disruptive and aggressive behaviors exhibited by Y. Nelson between January 15, 2023, and March 12, 2023. Investigations revealed that her actions incited public disturbances, financial losses to vendors, and significant distress to community members.\n",
      "-----------\n",
      "100139\n",
      "2005-01-01 00:00:00\n",
      "2820 - outdated by []\n",
      "- content:\n",
      "*Detention Measures Taken:*  \n",
      "On February 20, 2023, Y. Nelson was detained under suspicion of Picking Quarrels and Provoking Trouble after a series of disturbances. Detailed assessments of the collected evidence further implicated the defendant.\n",
      "\n",
      "*Criminal Detention:*  \n",
      "On February 22, 2023, the Quailwood Police Department placed Y. Nelson under formal criminal detention following additional evidence collection and verification.\n",
      "-----------\n",
      "100139\n",
      "2005-01-01 00:00:00\n",
      "2822 - outdated by []\n",
      "- content:\n",
      "During the period from January 15, 2023, to February 5, 2023, Y. Nelson engaged in numerous provocations and quarrels in public places such as the central market and cafes in downtown Quailwood. Witness testimonies and surveillance footage confirmed her involvement in these disruptions.\n",
      "-----------\n",
      "100139\n",
      "2005-01-01 00:00:00\n",
      "2834 - outdated by []\n",
      "- content:\n",
      "**Verdict Delivered:**\n",
      "\n",
      "June 10, 2023  \n",
      "Glenwood, Quailwood Court  \n",
      "\n",
      "*Chief Judge:* Hon. H. Ruiz  \n",
      "*Presiding Judge:* Hon. E. Collins  \n",
      "*Court Clerk:* K. Kelly\n",
      "-----------\n",
      "139\n",
      "2000-01-01 00:00:00\n",
      "2108 - outdated by []\n",
      "- content:\n",
      "Glenwood, Quailwood Court\n",
      "9th Judicial Circuit\n",
      "State of Glenwood\n",
      "\n",
      "Case No: 2023-458-CR\n",
      "\n",
      "Chief Judge: Hon. H. Ruiz\n",
      "Presiding Judge: Hon. E. Collins\n",
      "Court Clerk: K. Kelly\n",
      "\n",
      "---\n",
      "\n",
      "**JUDGMENT**\n",
      "\n",
      "**The People of Glenwood vs. Y. Nelson**\n",
      "\n",
      "**1. Court and Prosecutor Information:**\n",
      "\n",
      "*Court:* Glenwood, Quailwood Court  \n",
      "*Prosecutor:* Glenwood, Quailwood Procuratorate  \n",
      "\n",
      "*Chief Judge:* Hon. H. Ruiz  \n",
      "*Presiding Judge:* Hon. E. Collins  \n",
      "*Court Clerk:* K. Kelly  \n",
      "\n",
      "**2. Defendant and Defense Lawyer Information:**\n",
      "-----------\n",
      "139\n",
      "2000-01-01 00:00:00\n",
      "2112 - outdated by []\n",
      "- content:\n",
      "*Arrest:*  \n",
      "On March 15, 2023, the Glenwood, Quailwood Procuratorate issued an arrest warrant for Y. Nelson upon finding sufficient evidence to substantiate the charges.\n",
      "\n",
      "**4. Case Statement:**\n",
      "\n",
      "The case revolves around a series of disruptive and aggressive behaviors exhibited by Y. Nelson between January 15, 2023, and March 12, 2023. Investigations revealed that her actions incited public disturbances, financial losses to vendors, and significant distress to community members.\n",
      "-----------\n",
      "139\n",
      "2000-01-01 00:00:00\n",
      "2111 - outdated by []\n",
      "- content:\n",
      "*Detention Measures Taken:*  \n",
      "On February 20, 2023, Y. Nelson was detained under suspicion of Picking Quarrels and Provoking Trouble after a series of disturbances. Detailed assessments of the collected evidence further implicated the defendant.\n",
      "\n",
      "*Criminal Detention:*  \n",
      "On February 22, 2023, the Quailwood Police Department placed Y. Nelson under formal criminal detention following additional evidence collection and verification.\n",
      "-----------\n",
      "139\n",
      "2000-01-01 00:00:00\n",
      "2113 - outdated by []\n",
      "- content:\n",
      "During the period from January 15, 2023, to February 5, 2023, Y. Nelson engaged in numerous provocations and quarrels in public places such as the central market and cafes in downtown Quailwood. Witness testimonies and surveillance footage confirmed her involvement in these disruptions.\n",
      "-----------\n",
      "139\n",
      "2000-01-01 00:00:00\n",
      "2125 - outdated by []\n",
      "- content:\n",
      "**Verdict Delivered:**\n",
      "\n",
      "June 10, 2023  \n",
      "Glenwood, Quailwood Court  \n",
      "\n",
      "*Chief Judge:* Hon. H. Ruiz  \n",
      "*Presiding Judge:* Hon. E. Collins  \n",
      "*Court Clerk:* K. Kelly\n",
      "-----------\n",
      "Chef\n"
     ]
    }
   ],
   "source": [
    "for c in context:\n",
    "    print(c.metadata[\"doc_id\"])\n",
    "    print(c.metadata[\"creation_date\"])\n",
    "    print(f\"{c.metadata[\"chunk_id\"]} - outdated by {c.metadata[\"outdated_by_chunk_ids\"]}\")\n",
    "    print(\"- content:\")\n",
    "    print(c.page_content)\n",
    "    print(\"-----------\")\n",
    "\n",
    "print(response.answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
