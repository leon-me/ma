{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44f6acd",
   "metadata": {},
   "source": [
    "## Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ef9df",
   "metadata": {},
   "source": [
    "### Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7adc8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.04 %\t(with DQ assessment)\n",
      "62.25 %\t(without DQ assessment)\n"
     ]
    }
   ],
   "source": [
    "### Calculate Recall\n",
    "from langchain_core.load import loads\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def calculate_recall(\n",
    "    top_k: int, with_dq: bool = True, print_info_if_wrong: bool = False, emb: Literal[\"_large\", \"\"] = \"\"\n",
    "):\n",
    "    filename = (\n",
    "        f\"evaluation/_queries_with_context_with_DQ_{top_k}.csv\"\n",
    "        if with_dq\n",
    "        else f\"evaluation/_queries_with_context_without_DQ_{top_k}.csv\"\n",
    "    )\n",
    "    data = pd.read_csv(\n",
    "        filename,\n",
    "        converters={\"context\": ast.literal_eval, \"ground_truth.references\": ast.literal_eval},\n",
    "    )\n",
    "    data[\"context\"] = data[\"context\"].apply(lambda chunks: [loads(c) for c in chunks])\n",
    "\n",
    "    hits = []\n",
    "    for _, row in data.iterrows():\n",
    "        complete_context_txt = \"\\n\".join([chunk.page_content for chunk in row[\"context\"]]).lower()\n",
    "        hit_in_row = True\n",
    "        for ref in row[\"ground_truth.references\"]:\n",
    "            hit_in_row = hit_in_row and (ref.lower().strip() in complete_context_txt)\n",
    "            if print_info_if_wrong and (not (ref.lower().strip().strip(string.punctuation) in complete_context_txt)):\n",
    "                print(\"------\")\n",
    "                print(f\"{row[\"query.content\"]}: {row[\"ground_truth.content\"]}, {row[\"ground_truth.doc_ids\"]}\")\n",
    "                print(\"Reference:\")\n",
    "                print(ref.lower().strip())\n",
    "                print()\n",
    "                print(\"Metadata:\")\n",
    "                print(\"\\n\".join([str(c.metadata) for c in row[\"context\"]]))\n",
    "                print(\"Context:\")\n",
    "                print(\"\\n- \".join([c.page_content for c in row[\"context\"]]))\n",
    "        hits.append(hit_in_row)\n",
    "    recall = np.mean(hits)\n",
    "    return recall\n",
    "\n",
    "\n",
    "print(f\"{calculate_recall(5, with_dq=True)*100:.2f} %\\t(with DQ assessment)\")\n",
    "print(f\"{calculate_recall(5, with_dq=False)*100:.2f} %\\t(without DQ assessment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781d168",
   "metadata": {},
   "source": [
    "## Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37c126",
   "metadata": {},
   "source": [
    "### Completeness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True]\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate keypoint coverage estimation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import importlib\n",
    "from utils import llm, io_helpers\n",
    "\n",
    "importlib.reload(llm)\n",
    "\n",
    "filename = \"evaluation/_queries_with_context_with_DQ_5_generations.csv\"\n",
    "filename_new = \"evaluation/keypoint_eval/_queries_with_context_with_DQ_5_generations.csv\"\n",
    "\n",
    "\n",
    "def calc_keypoint_coverage(row, model: str = \"gpt-4.1\"):\n",
    "    question = row[\"query.content\"]\n",
    "    keypoints = row[\"ground_truth.keypoints\"]\n",
    "    generated_answer = row[\"generated_response\"]\n",
    "\n",
    "    system_prompt, user_prompt = io_helpers.get_prompts(\"keypoints/keypoints_validation\")\n",
    "    user_prompt = llm.format_user_prompt_keypoint_validation(\n",
    "        user_prompt, question=question, keypoints=keypoints, generated_answer=generated_answer\n",
    "    )\n",
    "\n",
    "    response = llm.call_any_llm(\n",
    "        system_prompt, user_prompt, model=model, response_format_pydantic=llm.LLMKeypointEvaluationResponse\n",
    "    )\n",
    "\n",
    "    return np.mean(response.keypoint_coverage).item()\n",
    "\n",
    "\n",
    "data = pd.read_csv(filename, converters={\"ground_truth.keypoints\": ast.literal_eval})\n",
    "data[\"keypoint_coverage\"] = data.apply(calc_keypoint_coverage, axis=1)\n",
    "\n",
    "data.to_csv(filename_new, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
