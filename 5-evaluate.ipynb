{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44f6acd",
   "metadata": {},
   "source": [
    "## Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ef9df",
   "metadata": {},
   "source": [
    "### Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adc8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.04 %\t(with DQ assessment)\n",
      "62.25 %\t(without DQ assessment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/qhsvkwd97tsbm3310qw2zl2w0000gn/T/ipykernel_32205/3107746574.py:22: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  data[\"context\"] = data[\"context\"].apply(lambda chunks: [loads(c) for c in chunks])\n"
     ]
    }
   ],
   "source": [
    "### Calculate Recall\n",
    "from langchain_core.load import loads\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def calculate_recall(\n",
    "    top_k: int, with_dq: bool = True, print_info_if_wrong: bool = False, emb: Literal[\"_large\", \"\"] = \"\"\n",
    "):\n",
    "    filename = (\n",
    "        f\"evaluation/_queries_with_context_with_DQ_{top_k}.csv\"\n",
    "        if with_dq\n",
    "        else f\"evaluation/_queries_with_context_without_DQ_{top_k}.csv\"\n",
    "    )\n",
    "    data = pd.read_csv(\n",
    "        filename,\n",
    "        converters={\"context\": ast.literal_eval, \"ground_truth.references\": ast.literal_eval},\n",
    "    )\n",
    "    data[\"context\"] = data[\"context\"].apply(lambda chunks: [loads(c) for c in chunks])\n",
    "\n",
    "    hits = []\n",
    "    for _, row in data.iterrows():\n",
    "        complete_context_txt = \"\\n\".join([chunk.page_content for chunk in row[\"context\"]]).lower()\n",
    "        hit_in_row = True\n",
    "        for ref in row[\"ground_truth.references\"]:\n",
    "            hit_in_row = hit_in_row and (ref.lower().strip() in complete_context_txt)\n",
    "            if print_info_if_wrong and (not (ref.lower().strip().strip(string.punctuation) in complete_context_txt)):\n",
    "                print(\"------\")\n",
    "                print(f\"{row[\"query.content\"]}: {row[\"ground_truth.content\"]}, {row[\"ground_truth.doc_ids\"]}\")\n",
    "                print(\"Reference:\")\n",
    "                print(ref.lower().strip())\n",
    "                print()\n",
    "                print(\"Metadata:\")\n",
    "                print(\"\\n\".join([str(c.metadata) for c in row[\"context\"]]))\n",
    "                print(\"Context:\")\n",
    "                print(\"\\n- \".join([c.page_content for c in row[\"context\"]]))\n",
    "        hits.append(hit_in_row)\n",
    "    recall = np.mean(hits)\n",
    "    return recall\n",
    "\n",
    "\n",
    "print(f\"{calculate_recall(5, with_dq=True)*100:.2f} %\\t(with DQ assessment)\")\n",
    "print(f\"{calculate_recall(5, with_dq=False)*100:.2f} %\\t(without DQ assessment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1e7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.21 %\t(with DQ assessment)\n",
      "13.14 %\t(without DQ assessment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/qhsvkwd97tsbm3310qw2zl2w0000gn/T/ipykernel_69514/2420060671.py:22: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  data[\"context\"] = data[\"context\"].apply(lambda chunks: [loads(c) for c in chunks])\n"
     ]
    }
   ],
   "source": [
    "### Calculate Precision\n",
    "from langchain_core.load import loads\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def calculate_precision(\n",
    "    top_k: int, with_dq: bool = True, print_info_if_wrong: bool = False, emb: Literal[\"_large\", \"\"] = \"\"\n",
    "):\n",
    "    filename = (\n",
    "        f\"evaluation/_queries_with_context_with_DQ_{top_k}.csv\"\n",
    "        if with_dq\n",
    "        else f\"evaluation/_queries_with_context_without_DQ_{top_k}.csv\"\n",
    "    )\n",
    "    data = pd.read_csv(\n",
    "        filename,\n",
    "        converters={\"context\": ast.literal_eval, \"ground_truth.references\": ast.literal_eval},\n",
    "    )\n",
    "    data[\"context\"] = data[\"context\"].apply(lambda chunks: [loads(c) for c in chunks])\n",
    "\n",
    "    precision_list = []\n",
    "    for _, row in data.iterrows():\n",
    "        precision_in_row = []\n",
    "        for chunk in row[\"context\"]:\n",
    "            chunk_relevant = False\n",
    "            for ref in row[\"ground_truth.references\"]:\n",
    "                if ref.lower().strip() in str(chunk.page_content).lower():\n",
    "                    chunk_relevant = True\n",
    "                    break\n",
    "            precision_in_row.append(chunk_relevant)\n",
    "        precision_list.append(np.mean(precision_in_row))\n",
    "    precision = np.mean(precision_list)\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "print(f\"{calculate_precision(5, with_dq=True)*100:.2f} %\\t(with DQ assessment)\")\n",
    "print(f\"{calculate_precision(5, with_dq=False)*100:.2f} %\\t(without DQ assessment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781d168",
   "metadata": {},
   "source": [
    "## Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37c126",
   "metadata": {},
   "source": [
    "### Completeness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec5bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leon/miniconda3/envs/ma/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Generate keypoint coverage estimation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import importlib\n",
    "from utils import llm, io_helpers\n",
    "\n",
    "importlib.reload(llm)\n",
    "\n",
    "\n",
    "def calc_keypoint_coverage(row, model: str = \"gpt-4.1\"):\n",
    "    question = row[\"query.content\"]\n",
    "    keypoints = row[\"ground_truth.keypoints\"]\n",
    "    generated_answer = row[\"generated_response\"]\n",
    "\n",
    "    system_prompt, user_prompt = io_helpers.get_prompts(\"keypoints/keypoints_validation\")\n",
    "    user_prompt = llm.format_user_prompt_keypoint_validation(\n",
    "        user_prompt, question=question, keypoints=keypoints, generated_answer=generated_answer\n",
    "    )\n",
    "\n",
    "    response = llm.call_any_llm(\n",
    "        system_prompt, user_prompt, model=model, response_format_pydantic=llm.LLMKeypointEvaluationResponse\n",
    "    )\n",
    "\n",
    "    return np.mean(response.keypoint_coverage).item()\n",
    "\n",
    "\n",
    "def generate_keypoint_coverage(filepath: str, filepath_new: str):\n",
    "    if filepath == filepath_new:\n",
    "        raise RuntimeError(\"Paths must differ!\")\n",
    "    data = pd.read_csv(filename, converters={\"ground_truth.keypoints\": ast.literal_eval})\n",
    "    data[\"keypoint_coverage\"] = data.apply(calc_keypoint_coverage, axis=1)\n",
    "    data.to_csv(filepath_new, index=False)\n",
    "\n",
    "\n",
    "filename = \"evaluation/_queries_with_context_without_DQ_5_generations.csv\"\n",
    "filename_new = \"evaluation/keypoint_eval/_queries_with_context_without_DQ_5_generations.csv\"\n",
    "# _ = generate_keypoint_coverage(filename, filename_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be9be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.11 %\t(with DQ assessment)\n",
      "27.98 %\t(without DQ assessment)\n"
     ]
    }
   ],
   "source": [
    "### Calculate Completeness\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filename_with_dq = \"evaluation/keypoint_eval/_queries_with_context_with_DQ_5_generations.csv\"\n",
    "filename_without_dq = \"evaluation/keypoint_eval/_queries_with_context_without_DQ_5_generations.csv\"\n",
    "\n",
    "completeness_with_dq = np.mean(\n",
    "    pd.read_csv(filename_with_dq, usecols=[\"keypoint_coverage\"], dtype={\"keypoint_coverage\": \"float64\"})[\n",
    "        \"keypoint_coverage\"\n",
    "    ]\n",
    ")\n",
    "completeness_without_dq = np.mean(\n",
    "    pd.read_csv(filename_without_dq, usecols=[\"keypoint_coverage\"], dtype={\"keypoint_coverage\": \"float64\"})[\n",
    "        \"keypoint_coverage\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"{completeness_with_dq*100:.2f} %\\t(with DQ assessment)\")\n",
    "print(f\"{completeness_without_dq*100:.2f} %\\t(without DQ assessment)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
