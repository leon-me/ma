{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6703517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"/Users/leon/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5ce678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### static variables\n",
    "\n",
    "COLUMNS_DOCS = [\n",
    "    \"doc_id\",\n",
    "    \"language\",\n",
    "    \"domain\",\n",
    "    \"content\",\n",
    "    \"company_name\",\n",
    "    \"court_name\",\n",
    "    \"hospital_patient_name\",\n",
    "]\n",
    "\n",
    "COLUMNS_DOCS_MANIPULATED_TEXTUAL = [\n",
    "    *COLUMNS_DOCS,\n",
    "    \"original_doc_id\",\n",
    "]\n",
    "\n",
    "COLUMNS_DOCS_MANIPULATED_TABULAR_ROW = [\n",
    "    *COLUMNS_DOCS,\n",
    "    \"original_doc_id\",\n",
    "    \"query.original_query_id\",\n",
    "    \"ground_truth.content\",\n",
    "]\n",
    "\n",
    "COLUMNS_DOCS_MANIPULATED_TABULAR = [\n",
    "    \"doc_id\",\n",
    "    \"language\",\n",
    "    \"domain\",\n",
    "    \"content\",\n",
    "    \"company_names\",\n",
    "    \"court_names\",\n",
    "    \"hospital_patient_names\",\n",
    "    \"original_doc_ids\",\n",
    "]\n",
    "\n",
    "COLUMNS_QUERIES = [\n",
    "    \"domain\",\n",
    "    \"ground_truth.content\",\n",
    "    \"ground_truth.doc_ids\",\n",
    "    \"ground_truth.keypoints\",\n",
    "    \"ground_truth.references\",\n",
    "    \"language\",\n",
    "    \"prediction\",\n",
    "    \"query.content\",\n",
    "    \"query.query_id\",\n",
    "    \"query.query_type\",\n",
    "]\n",
    "\n",
    "COLUMNS_QUERIES_MANIPULATED = [*COLUMNS_QUERIES, \"query.original_query_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4a800bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper functions/classes for manipulation\n",
    "from typing import Tuple, List, Dict\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "import os\n",
    "from typing import Literal\n",
    "import sys\n",
    "import json\n",
    "\n",
    "DOCUMENTS = pd.read_csv(\"DRAGONball/en/docs.csv\")\n",
    "QUERIES = pd.read_csv(\n",
    "    \"DRAGONball/en/queries_flattened.csv\",\n",
    "    converters={\n",
    "        \"ground_truth.doc_ids\": ast.literal_eval,\n",
    "        \"ground_truth.keypoints\": ast.literal_eval,\n",
    "        \"ground_truth.references\": ast.literal_eval,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "class SingleTextualManipulationResponse(BaseModel):\n",
    "    text_new: str\n",
    "    answer_new: str\n",
    "    references_new: list[str]\n",
    "\n",
    "class QAPair(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    references: List[str]\n",
    "\n",
    "class MultiTextualManipulationResponse(BaseModel):\n",
    "    text_new: str\n",
    "    qa_pairs_new: List[QAPair]\n",
    "\n",
    "class SingleTabularManipulationResponse(BaseModel):\n",
    "    answer_new: str\n",
    "    description: str\n",
    "    value: str\n",
    "\n",
    "def read_prompt(path: str | os.PathLike) -> Dict:\n",
    "    \"\"\"Reads from JSON-file\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def format_user_prompt_single_textual(user_prompt: str, text: str, question: str, answer: str, references: str) -> str:\n",
    "    return user_prompt.format(text=text, question=question, answer=answer, references=references)\n",
    "\n",
    "\n",
    "def format_user_prompt_single_tabular(user_prompt: str, question: str, answer: str, entity: str) -> str:\n",
    "    return user_prompt.format(question=question, answer=answer, entity=entity)\n",
    "\n",
    "def format_user_prompt_multi_textual(user_prompt: str, text: str, qa_pairs: List[Dict]) -> str:\n",
    "    \"\"\"qa_pairs must be of format: [ { question: '...', answer: '...' }, ... ]\"\"\"\n",
    "    questions_str = \"\"\n",
    "    for id, qa in enumerate(qa_pairs, start=1):\n",
    "        questions_str += f\"question_{id}: {qa[\"question\"]}\\n\"\n",
    "        questions_str += f\"answer_{id}: {qa[\"answer\"]}\\n\"\n",
    "\n",
    "    return user_prompt.format(text=text, questions=questions_str)\n",
    "\n",
    "\n",
    "def get_query_ids_for_doc(doc_id: int | str, query_types: list[str] = [\"Factual Question\"]) -> list[int]:\n",
    "    \"\"\"Selects query_ids for queries related to that doc and with a specified type.\"\"\"\n",
    "    return QUERIES[\n",
    "        QUERIES[\"ground_truth.doc_ids\"].apply(lambda doc_ids: int(doc_id) in doc_ids)\n",
    "        & QUERIES[\"query.query_type\"].isin(query_types)\n",
    "    ][\"query.query_id\"].to_list()\n",
    "\n",
    "\n",
    "def get_doc_query_mapping_single(target: Literal[\"textual\", \"tabular\"]) -> List[Dict[str, int]]:\n",
    "    with open(\"doc_query_mapping_single.csv\", \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            {\n",
    "                \"doc_id\": int(row[\"doc_id\"]),\n",
    "                \"query_id\": int(row[\"query_id_single\"]) if target == \"textual\" else int(row[\"query_id_multi\"]),\n",
    "            }\n",
    "            for row in reader\n",
    "        ]\n",
    "\n",
    "def get_doc_query_mapping_multi() -> List[Dict]:\n",
    "    with open(\"docs_to_manipulate.csv\", \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        doc_ids = [row[\"doc_id\"] for row in reader if int(row[\"multi_textual_manipulation\"]) == 1]\n",
    "    mapping = []\n",
    "    for id in doc_ids: \n",
    "        entry = {\n",
    "            \"doc_id\": id,\n",
    "            \"query_ids\": get_query_ids_for_doc(id, query_types=[\"Factual Question\"])\n",
    "            }\n",
    "        mapping.append(entry)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def get_query_properties(\n",
    "    query_id,\n",
    "    properties: list = [\"ground_truth.content\", \"ground_truth.keypoints\", \"ground_truth.references\", \"query.content\"],\n",
    ") -> Tuple:\n",
    "    \"\"\"Select columns for query_id from queries dataframe.\"\"\"\n",
    "    row = QUERIES[QUERIES[\"query.query_id\"] == query_id]\n",
    "    return tuple(row[prop].iloc[0] for prop in properties)\n",
    "\n",
    "\n",
    "def get_doc_properties(\n",
    "    doc_id,\n",
    "    properties,\n",
    ") -> Tuple:\n",
    "    \"\"\"Select columns for doc_id from docs dataframe.\"\"\"\n",
    "    row: pd.DataFrame = DOCUMENTS[DOCUMENTS[\"doc_id\"] == doc_id].dropna(axis=1)\n",
    "    return tuple(row[prop].iloc[0] for prop in properties if prop in row.columns)\n",
    "\n",
    "\n",
    "def get_doc_text(doc_id: int | str) -> str:\n",
    "    return DOCUMENTS[DOCUMENTS[\"doc_id\"] == int(doc_id)][\"content\"].iloc[0]\n",
    "\n",
    "\n",
    "def get_query_by_id(query_id: int) -> pd.Series:\n",
    "    return QUERIES[QUERIES[\"query.query_id\"].astype(int) == query_id].iloc[0]\n",
    "\n",
    "\n",
    "def get_queries_by_id(query_ids: List[int]) -> pd.DataFrame:\n",
    "    return QUERIES[QUERIES[\"query.query_id\"].astype(int).isin(query_ids)]\n",
    "\n",
    "\n",
    "def get_doc_by_id(doc_id: int | str) -> pd.Series:\n",
    "    return DOCUMENTS[DOCUMENTS[\"doc_id\"].astype(int) == int(doc_id)].iloc[0]\n",
    "\n",
    "\n",
    "def openai_interface(system_prompt, user_prompt, response_format_pydantic=SingleTextualManipulationResponse, temperature: float = 0.0):\n",
    "    \"\"\"execute openai LLM call\"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    print(\"DEBUG: Attempting LLM call\")\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        response_format=response_format_pydantic,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    print(f\"DEBUG: Finished LLM call. Usage: {completion.usage.prompt_tokens} prompt tokens and {completion.usage.completion_tokens} completion tokens.\")\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "\n",
    "def get_prompts_for_textual_single(doc_id: int, query_id: int) -> Tuple:\n",
    "    PROMPT_TYPE = \"manipulation_factual\"\n",
    "    prompt = [\n",
    "        prompt for prompt in read_prompt(\"prompts/json/manipulate_docs.json\") if prompt[\"prompt_type\"] == PROMPT_TYPE\n",
    "    ][0]\n",
    "    text = get_doc_text(doc_id)\n",
    "    answer, keypoints, references, question = get_query_properties(query_id)\n",
    "    system_prompt = prompt[\"system_prompt\"]\n",
    "    user_prompt = format_user_prompt_single_textual(\n",
    "        user_prompt=prompt[\"user_prompt\"], text=text, answer=answer, question=question, references=references\n",
    "    )\n",
    "    return (system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "def get_prompts_for_tabular_single(query_id: int, doc_id: int) -> Tuple:\n",
    "    prompt_obj = read_prompt(\"prompts/json/manipulation_tabular.json\")\n",
    "    system_prompt = prompt_obj[\"system_prompt\"]\n",
    "    user_prompt = prompt_obj[\"user_prompt\"]\n",
    "    answer, question = get_query_properties(query_id, properties=[\"ground_truth.content\", \"query.content\"])\n",
    "    (entity,) = get_doc_properties(doc_id, [\"hospital_patient_name\", \"company_name\", \"court_name\"])\n",
    "    user_prompt = format_user_prompt_single_tabular(user_prompt, question, answer, entity)\n",
    "    return (system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "def get_prompts_for_textual_multi(doc_id: int, query_ids: List[int]) -> Tuple:\n",
    "    prompt_obj = read_prompt(\"../prompts/json/manipulation_multi_textual.json\")\n",
    "    text = get_doc_text(doc_id)\n",
    "    qa_pairs = []\n",
    "    for query_id in query_ids:\n",
    "        question, answer = get_query_properties(query_id, [\"query.content\",\"ground_truth.content\"])\n",
    "        qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "    \n",
    "    system_prompt = prompt_obj[\"system_prompt\"]\n",
    "    user_prompt = format_user_prompt_multi_textual(user_prompt=prompt_obj[\"user_prompt\"], text=text, qa_pairs=qa_pairs)\n",
    "    \n",
    "    return (system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "def get_id_for_manipulated_doc_or_query(original_doc_id: int, prefix_number=1) -> int:\n",
    "    id_str = str(prefix_number) + str(original_doc_id).zfill(5)\n",
    "    return int(id_str)\n",
    "\n",
    "\n",
    "def save_manipulated_doc(filename: os.PathLike | str, fieldnames: List[str], **kwargs):\n",
    "    \"\"\"Saves a manipulated doc to csv.\n",
    "    If an entry with that doc_id already exists in the csv, the new entry is NOT saved.\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        raise RuntimeError(\"Must specify a filename!\")\n",
    "    \n",
    "    print(f\"Saving Doc with ID '{kwargs[\"doc_id\"]}'\")\n",
    "    is_empty = not os.path.exists(filename) or os.stat(filename).st_size == 0\n",
    "    id_exists = False\n",
    "    with open(filename, \"a+\", newline=\"\") as f:\n",
    "        if not is_empty:\n",
    "            f.seek(0)\n",
    "            reader = csv.DictReader(f, fieldnames=fieldnames)\n",
    "            ids_present = {int(row[\"doc_id\"]) for row in list(reader)[1:]}\n",
    "            id_exists = int(kwargs[\"doc_id\"]) in ids_present\n",
    "\n",
    "        if id_exists == True:\n",
    "            print(f\"WARN: Row with ID {kwargs[\"doc_id\"]} already exists. Did not write new document to '{filename}'.\")\n",
    "            return\n",
    "\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        if is_empty:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(kwargs)\n",
    "\n",
    "\n",
    "def save_manipulated_query(\n",
    "    filename: os.PathLike | str,\n",
    "    fieldnames: List[str],\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Saves a manipulated query to csv.\n",
    "    If an entry with that doc_id already exists in the csv, the new entry is NOT saved.\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        raise RuntimeError(\"Must specify a filename!\")\n",
    "    \n",
    "    print(f\"Saving Query with ID '{kwargs[\"query.query_id\"]}'\")\n",
    "    is_empty = not os.path.exists(filename) or os.stat(filename).st_size == 0\n",
    "    id_exists = False\n",
    "    with open(filename, \"a+\", newline=\"\") as f:\n",
    "        if not is_empty:\n",
    "            f.seek(0)\n",
    "            reader = csv.DictReader(f, fieldnames=fieldnames)\n",
    "            ids_present = {int(row[\"query.query_id\"]) for row in list(reader)[1:]}\n",
    "            id_exists = int(kwargs[\"query.query_id\"]) in ids_present\n",
    "\n",
    "        if id_exists == True:\n",
    "            print(f\"WARN: Row with ID {kwargs[\"query.query_id\"]} already exists. Did not write new query to '{filename}'.\")\n",
    "            return\n",
    "\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        if is_empty:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9ef2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:\n",
      " Index(['hospital_patient_name', 'language', 'doc_id', 'domain', 'content',\n",
      "       'company_name', 'court_name'],\n",
      "      dtype='object')\n",
      "\n",
      "Queries:\n",
      " Index(['domain', 'ground_truth.content', 'ground_truth.doc_ids',\n",
      "       'ground_truth.keypoints', 'ground_truth.references', 'language',\n",
      "       'prediction', 'query.content', 'query.query_id', 'query.query_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### describe documents and queries\n",
    "\n",
    "print(\"Documents:\\n\", DOCUMENTS.columns)\n",
    "print()\n",
    "print(\"Queries:\\n\", QUERIES.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205f0e4",
   "metadata": {},
   "source": [
    "## Single Textual Manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### manipulate documents (textual)\n",
    "def save_single_textual(doc_entry, query_entry, completion_parsed):\n",
    "    # -- doc\n",
    "    original_doc_id = doc_entry.doc_id\n",
    "    manipulated_doc_entry = doc_entry.copy()\n",
    "    manipulated_doc_entry.doc_id = get_id_for_manipulated_doc_or_query(original_doc_id)\n",
    "    manipulated_doc_entry.content = completion_parsed.text_new\n",
    "    manipulated_doc_entry = pd.concat([manipulated_doc_entry, pd.Series([original_doc_id], index=[\"original_doc_id\"])])\n",
    "\n",
    "    save_manipulated_doc(**manipulated_doc_entry)\n",
    "\n",
    "    # -- query\n",
    "    original_query_id = query_entry[\"query.query_id\"]\n",
    "    manipulated_query_entry = query_entry.copy()\n",
    "    manipulated_query_entry[\"ground_truth.content\"] = completion_parsed.answer_new\n",
    "    manipulated_query_entry[\"ground_truth.references\"] = completion_parsed.references_new\n",
    "    manipulated_query_entry[\"ground_truth.keypoints\"] = []\n",
    "    manipulated_query_entry[\"query.query_id\"] = get_id_for_manipulated_doc_or_query(original_query_id)\n",
    "    manipulated_query_entry = pd.concat(\n",
    "        [manipulated_query_entry, pd.Series([original_query_id], index=[\"query.original_query_id\"])]\n",
    "    )\n",
    "    manipulated_query_entry.index = manipulated_query_entry.index.str.replace(\".\", \"__\", regex=False)\n",
    "\n",
    "    save_manipulated_query(**manipulated_query_entry)\n",
    "\n",
    "\n",
    "doc_query_mapping = get_doc_query_mapping_single()\n",
    "\n",
    "for mapping in doc_query_mapping:\n",
    "    doc_id = mapping[\"doc_id\"]\n",
    "    query_id = mapping[\"query_id\"]\n",
    "\n",
    "    doc_entry = get_doc_by_id(doc_id)\n",
    "    query_entry = get_query_by_id(query_id)\n",
    "\n",
    "    system_prompt, user_prompt = get_prompts_for_textual_single(doc_id, query_id)\n",
    "\n",
    "    # call openai\n",
    "    completion_parsed = openai_interface(system_prompt, user_prompt)\n",
    "\n",
    "    save_single_textual(doc_entry, query_entry, completion_parsed)\n",
    "    print(f\"Finished processing doc {doc_id} and query {query_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd70e1",
   "metadata": {},
   "source": [
    "## Single Tabular Manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### manipulate documents (tabular) and save rows\n",
    "mapping = get_doc_query_mapping_single(\"tabular\")\n",
    "\n",
    "doc_entries = []\n",
    "\n",
    "for id_pair in mapping:\n",
    "    doc_id = id_pair[\"doc_id\"]\n",
    "    query_id = id_pair[\"query_id\"]\n",
    "    system_prompt, user_prompt = get_prompts_for_tabular_single(query_id, doc_id)\n",
    "\n",
    "    response: SingleTabularManipulationResponse = openai_interface(\n",
    "        system_prompt, user_prompt, SingleTabularManipulationResponse\n",
    "    )\n",
    "\n",
    "    manipulated_doc_entry = get_doc_by_id(doc_id).copy()\n",
    "    manipulated_doc_entry.doc_id = get_id_for_manipulated_doc_or_query(doc_id, prefix_number=2)\n",
    "    manipulated_doc_entry.content = \" | \".join([response.description, response.value])\n",
    "    additional_fields = pd.Series(\n",
    "        [doc_id, query_id, response.answer_new],\n",
    "        index=[\"original_doc_id\", \"query.original_query_id\", \"ground_truth.content\"],\n",
    "    )\n",
    "    manipulated_doc_entry = pd.concat([manipulated_doc_entry, additional_fields])\n",
    "\n",
    "    save_manipulated_doc(\n",
    "        filename=\"additional_data/docs/tabular_manipulations_result_rows.csv\",\n",
    "        fieldnames=COLUMNS_DOCS_MANIPULATED_TABULAR_ROW,\n",
    "        **manipulated_doc_entry,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "### aggregate tabular rows and save docs\n",
    "tabular_docs = pd.read_csv(\"additional_data/docs/tabular_manipulations_result_rows.csv\")\n",
    "\n",
    "\n",
    "def list_or_none(series):\n",
    "    if series.dropna().empty:\n",
    "        return None\n",
    "    return series.tolist()\n",
    "\n",
    "\n",
    "agg_funcs = {\n",
    "    \"doc_id\": lambda x: 0,\n",
    "    \"language\": \"first\",\n",
    "    \"content\": \"\\n\".join,\n",
    "    \"company_name\": list_or_none,\n",
    "    \"court_name\": list_or_none,\n",
    "    \"hospital_patient_name\": list_or_none,\n",
    "    \"original_doc_id\": list_or_none,\n",
    "    \"query.original_query_id\": lambda x: None,\n",
    "    \"ground_truth.content\": lambda x: None,\n",
    "}\n",
    "\n",
    "aggregation = tabular_docs.groupby(\"domain\").agg(agg_funcs).reset_index()\n",
    "aggregation[\"doc_id\"] = [get_id_for_manipulated_doc_or_query(id, prefix_number=3) for id in [1, 2, 3]]\n",
    "aggregation.rename(\n",
    "    columns={\n",
    "        \"company_name\": \"company_names\",\n",
    "        \"court_name\": \"court_names\",\n",
    "        \"hospital_patient_name\": \"hospital_patient_names\",\n",
    "        \"original_doc_id\": \"original_doc_ids\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "for row_dict in aggregation.to_dict(orient=\"records\"):\n",
    "    save_manipulated_doc(\n",
    "        \"additional_data/docs/tabular_manipulations_result.csv\",\n",
    "        fieldnames=COLUMNS_DOCS_MANIPULATED_TABULAR,\n",
    "        **row_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d16476",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save manipulated queries for aggregated tabular docs\n",
    "tabular_docs = pd.read_csv(\"additional_data/docs/tabular_manipulations_result_rows.csv\")\n",
    "\n",
    "mapping_domain_doc_id = pd.read_csv(\n",
    "    \"additional_data/docs/tabular_manipulations_result.csv\", usecols=[\"domain\", \"doc_id\"]\n",
    ")\n",
    "mapping_dict = mapping_domain_doc_id.set_index(\"domain\")[\"doc_id\"].to_dict()\n",
    "\n",
    "for row_dict in tabular_docs.to_dict(orient=\"records\"):\n",
    "    original_query_id = row_dict[\"query.original_query_id\"]\n",
    "    manipulated_query_entry = get_query_by_id(original_query_id).copy()\n",
    "\n",
    "    manipulated_query_entry[\"ground_truth.doc_ids\"] = [mapping_dict[row_dict[\"domain\"]]]\n",
    "    manipulated_query_entry[\"ground_truth.content\"] = row_dict[\"ground_truth.content\"]\n",
    "    manipulated_query_entry[\"ground_truth.references\"] = row_dict[\"content\"]\n",
    "    manipulated_query_entry[\"ground_truth.keypoints\"] = []\n",
    "    manipulated_query_entry[\"query.query_id\"] = get_id_for_manipulated_doc_or_query(original_query_id, prefix_number=3)\n",
    "    manipulated_query_entry = pd.concat(\n",
    "        [manipulated_query_entry, pd.Series([original_query_id], index=[\"query.original_query_id\"])]\n",
    "    )\n",
    "\n",
    "    save_manipulated_query(\n",
    "        filename=\"additional_data/queries/tabular_manipulations_result.csv\",\n",
    "        fieldnames=COLUMNS_QUERIES_MANIPULATED,\n",
    "        **manipulated_query_entry\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4244b02",
   "metadata": {},
   "source": [
    "## Multi Textual Manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afac5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_multi_textual(doc_entry, query_entries: pd.DataFrame, text_new: str, qa_pairs: List[QAPair]):\n",
    "    # -- doc\n",
    "    original_doc_id = doc_entry[\"doc_id\"]\n",
    "    manipulated_doc_entry = doc_entry.copy()\n",
    "    manipulated_doc_entry[\"doc_id\"] = get_id_for_manipulated_doc_or_query(original_doc_id, prefix_number=4)\n",
    "    manipulated_doc_entry[\"content\"] = text_new\n",
    "    manipulated_doc_entry = pd.concat([manipulated_doc_entry, pd.Series([original_doc_id], index=[\"original_doc_id\"])])\n",
    "\n",
    "    save_manipulated_doc(\n",
    "        filename=\"additional_data/docs/multi_textual_manipulations.csv\",\n",
    "        fieldnames=COLUMNS_DOCS_MANIPULATED_TEXTUAL,\n",
    "        **manipulated_doc_entry,\n",
    "    )\n",
    "\n",
    "    # -- queries\n",
    "    for qa_pair in qa_pairs:\n",
    "        try:\n",
    "            query_entry = query_entries[\n",
    "                query_entries[\"query.content\"].apply(str.strip).apply(str.lower) == qa_pair.question.strip().lower()\n",
    "            ].iloc[0]\n",
    "        except IndexError:\n",
    "            raise RuntimeError(\"No matching question found!\")\n",
    "\n",
    "        original_query_id = query_entry[\"query.query_id\"]\n",
    "        manipulated_query_entry = query_entry.copy()\n",
    "        manipulated_query_entry[\"ground_truth.content\"] = qa_pair.answer\n",
    "        manipulated_query_entry[\"ground_truth.references\"] = qa_pair.references\n",
    "        manipulated_query_entry[\"ground_truth.keypoints\"] = []\n",
    "        manipulated_query_entry[\"ground_truth.doc_ids\"] = [manipulated_doc_entry[\"doc_id\"]]\n",
    "        manipulated_query_entry[\"query.query_id\"] = get_id_for_manipulated_doc_or_query(\n",
    "            original_query_id, prefix_number=4\n",
    "        )\n",
    "        manipulated_query_entry = pd.concat(\n",
    "            [manipulated_query_entry, pd.Series([original_query_id], index=[\"query.original_query_id\"])]\n",
    "        )\n",
    "\n",
    "        save_manipulated_query(\n",
    "            filename=\"additional_data/queries/multi_textual_manipulations.csv\",\n",
    "            fieldnames=COLUMNS_QUERIES_MANIPULATED,\n",
    "            **manipulated_query_entry,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d6c6a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1/30 Start processing document with ID '202' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 2/30 Start processing document with ID '128' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 3/30 Start processing document with ID '132' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 4/30 Start processing document with ID '133' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 5/30 Start processing document with ID '134' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 6/30 Start processing document with ID '136' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 7/30 Start processing document with ID '205' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 8/30 Start processing document with ID '139' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 9/30 Start processing document with ID '40' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 10/30 Start processing document with ID '42' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 11/30 Start processing document with ID '52' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 12/30 Start processing document with ID '53' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 13/30 Start processing document with ID '183' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 14/30 Start processing document with ID '59' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 15/30 Start processing document with ID '188' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 16/30 Start processing document with ID '192' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 17/30 Start processing document with ID '65' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 18/30 Start processing document with ID '194' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 19/30 Start processing document with ID '198' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 20/30 Start processing document with ID '199' ---\n",
      "Finished processing. Document has already been manipulated.\n",
      "--- 21/30 Start processing document with ID '74' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 5879 prompt tokens and 898 completion tokens.\n",
      "Saving Doc with ID '400202'\n",
      "Saving Query with ID '406174'\n",
      "Saving Query with ID '406175'\n",
      "Saving Query with ID '406176'\n",
      "Saving Query with ID '406177'\n",
      "Finished processing document with ID '202'. Changed 4 queries.\n",
      "--- 22/30 Start processing document with ID '202' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 5918 prompt tokens and 982 completion tokens.\n",
      "Saving Doc with ID '400204'\n",
      "Saving Query with ID '406197'\n",
      "Saving Query with ID '406198'\n",
      "Saving Query with ID '406199'\n",
      "Saving Query with ID '406200'\n",
      "Finished processing document with ID '204'. Changed 4 queries.\n",
      "--- 23/30 Start processing document with ID '204' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 6332 prompt tokens and 1181 completion tokens.\n",
      "Saving Doc with ID '400075'\n",
      "Saving Query with ID '403129'\n",
      "Saving Query with ID '403130'\n",
      "Saving Query with ID '403131'\n",
      "Saving Query with ID '403132'\n",
      "Saving Query with ID '403141'\n",
      "Saving Query with ID '403142'\n",
      "Saving Query with ID '403143'\n",
      "Saving Query with ID '403144'\n",
      "Saving Query with ID '403145'\n",
      "Finished processing document with ID '75'. Changed 9 queries.\n",
      "--- 24/30 Start processing document with ID '75' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 6504 prompt tokens and 1010 completion tokens.\n",
      "Saving Doc with ID '400078'\n",
      "Saving Query with ID '402817'\n",
      "Saving Query with ID '402818'\n",
      "Saving Query with ID '402831'\n",
      "Saving Query with ID '402832'\n",
      "Saving Query with ID '402833'\n",
      "Finished processing document with ID '78'. Changed 5 queries.\n",
      "--- 25/30 Start processing document with ID '78' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 6756 prompt tokens and 857 completion tokens.\n",
      "Saving Doc with ID '400079'\n",
      "Saving Query with ID '402710'\n",
      "Saving Query with ID '402711'\n",
      "Saving Query with ID '402727'\n",
      "Saving Query with ID '402728'\n",
      "Saving Query with ID '402729'\n",
      "Saving Query with ID '402730'\n",
      "Finished processing document with ID '79'. Changed 6 queries.\n",
      "--- 26/30 Start processing document with ID '79' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 5862 prompt tokens and 909 completion tokens.\n",
      "Saving Doc with ID '400207'\n",
      "Saving Query with ID '406213'\n",
      "Saving Query with ID '406214'\n",
      "Saving Query with ID '406215'\n",
      "Saving Query with ID '406216'\n",
      "Finished processing document with ID '207'. Changed 4 queries.\n",
      "--- 27/30 Start processing document with ID '207' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 5896 prompt tokens and 943 completion tokens.\n",
      "Saving Doc with ID '400213'\n",
      "Saving Query with ID '406257'\n",
      "Saving Query with ID '406258'\n",
      "Saving Query with ID '406259'\n",
      "Saving Query with ID '406260'\n",
      "Finished processing document with ID '213'. Changed 4 queries.\n",
      "--- 28/30 Start processing document with ID '213' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 5673 prompt tokens and 961 completion tokens.\n",
      "Saving Doc with ID '400214'\n",
      "Saving Query with ID '406283'\n",
      "Saving Query with ID '406284'\n",
      "Saving Query with ID '406285'\n",
      "Saving Query with ID '406286'\n",
      "Finished processing document with ID '214'. Changed 4 queries.\n",
      "--- 29/30 Start processing document with ID '214' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 6627 prompt tokens and 877 completion tokens.\n",
      "Saving Doc with ID '400110'\n",
      "Saving Query with ID '404380'\n",
      "Saving Query with ID '404381'\n",
      "Saving Query with ID '404382'\n",
      "Finished processing document with ID '110'. Changed 3 queries.\n",
      "--- 30/30 Start processing document with ID '110' ---\n",
      "DEBUG: Attempting LLM call\n",
      "DEBUG: Finished LLM call. Usage: 6608 prompt tokens and 966 completion tokens.\n",
      "Saving Doc with ID '400116'\n",
      "Saving Query with ID '404435'\n",
      "Saving Query with ID '404436'\n",
      "Saving Query with ID '404437'\n",
      "Saving Query with ID '404438'\n",
      "Finished processing document with ID '116'. Changed 4 queries.\n"
     ]
    }
   ],
   "source": [
    "doc_queries_mapping = get_doc_query_mapping_multi()\n",
    "\n",
    "try:\n",
    "    with open(\"additional_data/docs/multi_textual_manipulations.csv\", \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        doc_ids_present = set([int(row[\"doc_id\"]) for row in reader])\n",
    "except FileNotFoundError:\n",
    "    doc_ids_present = set()\n",
    "\n",
    "for index, mapping in enumerate(doc_queries_mapping, start=1):\n",
    "    print(f\"--- {index}/{len(doc_queries_mapping)} Start processing document with ID '{doc_id}' ---\")\n",
    "\n",
    "    doc_id = mapping[\"doc_id\"]\n",
    "    query_ids = mapping[\"query_ids\"]\n",
    "\n",
    "    if get_id_for_manipulated_doc_or_query(doc_id, prefix_number=4) in doc_ids_present:\n",
    "        print(\"Finished processing. Document has already been manipulated.\")\n",
    "        continue\n",
    "\n",
    "    doc_entry = get_doc_by_id(doc_id)\n",
    "    query_entries = QUERIES.loc[QUERIES[\"query.query_id\"].isin(query_ids)]\n",
    "\n",
    "    system_prompt, user_prompt = get_prompts_for_textual_multi(doc_id, query_ids)\n",
    "\n",
    "    # call openai\n",
    "    completion_parsed: MultiTextualManipulationResponse = openai_interface(\n",
    "        system_prompt, user_prompt, MultiTextualManipulationResponse, temperature=0.8\n",
    "    )\n",
    "\n",
    "    save_multi_textual(\n",
    "        doc_entry, query_entries, text_new=completion_parsed.text_new, qa_pairs=completion_parsed.qa_pairs_new\n",
    "    )\n",
    "    print(f\"Finished processing document with ID '{doc_id}'. Changed {len(query_ids)} queries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
