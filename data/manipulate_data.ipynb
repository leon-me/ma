{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6703517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"/Users/leon/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ce678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### static variables\n",
    "\n",
    "COLUMNS_DOCS = [\n",
    "    \"doc_id\",\n",
    "    \"language\",\n",
    "    \"domain\",\n",
    "    \"content\",\n",
    "    \"company_name\",\n",
    "    \"court_name\",\n",
    "    \"hospital_patient_name\",\n",
    "]\n",
    "\n",
    "COLUMNS_DOCS_MANIPULATED_TEXTUAL = [\n",
    "    *COLUMNS_DOCS,\n",
    "    \"original_doc_id\",\n",
    "]\n",
    "\n",
    "COLUMNS_DOCS_MANIPULATED_TABULAR_ROW = [\n",
    "    *COLUMNS_DOCS,\n",
    "    \"original_doc_id\",\n",
    "    \"query.original_query_id\",\n",
    "    \"ground_truth.content\",\n",
    "]\n",
    "\n",
    "COLUMNS_DOCS_MANIPULATED_TABULAR = [\n",
    "    \"doc_id\",\n",
    "    \"language\",\n",
    "    \"domain\",\n",
    "    \"content\",\n",
    "    \"company_names\",\n",
    "    \"court_names\",\n",
    "    \"hospital_patient_names\",\n",
    "    \"original_doc_ids\",\n",
    "]\n",
    "\n",
    "COLUMNS_QUERIES = [\n",
    "    \"domain\",\n",
    "    \"ground_truth.content\",\n",
    "    \"ground_truth.doc_ids\",\n",
    "    \"ground_truth.keypoints\",\n",
    "    \"ground_truth.references\",\n",
    "    \"language\",\n",
    "    \"prediction\",\n",
    "    \"query.content\",\n",
    "    \"query.query_id\",\n",
    "    \"query.query_type\",\n",
    "]\n",
    "\n",
    "COLUMNS_QUERIES_MANIPULATED = [*COLUMNS_QUERIES, \"query.original_query_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a800bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper functions/classes for manipulation\n",
    "from typing import Tuple, List, Dict\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "import os\n",
    "from typing import Literal\n",
    "import sys\n",
    "import json\n",
    "\n",
    "DOCUMENTS = pd.read_csv(\"DRAGONball/en/docs.csv\")\n",
    "QUERIES = pd.read_csv(\n",
    "    \"DRAGONball/en/queries_flattened.csv\",\n",
    "    converters={\n",
    "        \"ground_truth.doc_ids\": ast.literal_eval,\n",
    "        \"ground_truth.keypoints\": ast.literal_eval,\n",
    "        \"ground_truth.references\": ast.literal_eval,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "class FactualQuestionResponse(BaseModel):\n",
    "    text_new: str\n",
    "    answer_new: str\n",
    "    references_new: list[str]\n",
    "\n",
    "\n",
    "class TabularDataResponse(BaseModel):\n",
    "    answer_new: str\n",
    "    description: str\n",
    "    value: str\n",
    "\n",
    "def read_prompt(path: str | os.PathLike) -> Dict:\n",
    "    \"\"\"Reads from JSON-file\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def format_user_prompt_textual(user_prompt: str, text: str, question: str, answer: str, references: str) -> str:\n",
    "    return user_prompt.format(text=text, question=question, answer=answer, references=references)\n",
    "\n",
    "\n",
    "def format_user_prompt_tabular(user_prompt: str, question: str, answer: str, entity: str) -> str:\n",
    "    return user_prompt.format(question=question, answer=answer, entity=entity)\n",
    "\n",
    "\n",
    "def get_query_ids_for_doc(doc_id: int, query_types: list[str] = [\"Factual Question\"]) -> list[int]:\n",
    "    \"\"\"Selects query_ids for queries related to that doc and with a specified type.\"\"\"\n",
    "    return QUERIES[\n",
    "        QUERIES[\"ground_truth.doc_ids\"].apply(lambda doc_ids: doc_id in doc_ids)\n",
    "        & QUERIES[\"query.query_type\"].isin(query_types)\n",
    "    ][\"query.query_id\"].to_list()\n",
    "\n",
    "\n",
    "def get_doc_query_mapping(target: Literal[\"textual\", \"tabular\"]) -> List[Dict[str, int]]:\n",
    "    with open(\"doc_query_mapping_multi.csv\", \"r\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [\n",
    "            {\n",
    "                \"doc_id\": int(row[\"doc_id\"]),\n",
    "                \"query_id\": int(row[\"query_id_single\"]) if target == \"textual\" else int(row[\"query_id_multi\"]),\n",
    "            }\n",
    "            for row in reader\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_query_properties(\n",
    "    query_id,\n",
    "    properties: list = [\"ground_truth.content\", \"ground_truth.keypoints\", \"ground_truth.references\", \"query.content\"],\n",
    ") -> Tuple:\n",
    "    \"\"\"Select columns for query_id from queries dataframe.\"\"\"\n",
    "    row = QUERIES[QUERIES[\"query.query_id\"] == query_id]\n",
    "    return tuple(row[prop].iloc[0] for prop in properties)\n",
    "\n",
    "\n",
    "def get_doc_properties(\n",
    "    doc_id,\n",
    "    properties,\n",
    ") -> Tuple:\n",
    "    \"\"\"Select columns for doc_id from docs dataframe.\"\"\"\n",
    "    row: pd.DataFrame = DOCUMENTS[DOCUMENTS[\"doc_id\"] == doc_id].dropna(axis=1)\n",
    "    return tuple(row[prop].iloc[0] for prop in properties if prop in row.columns)\n",
    "\n",
    "\n",
    "def get_doc_text(doc_id: int) -> str:\n",
    "    return DOCUMENTS[DOCUMENTS[\"doc_id\"] == doc_id][\"content\"].iloc[0]\n",
    "\n",
    "\n",
    "def get_query_by_id(query_id: int) -> pd.Series:\n",
    "    return QUERIES[QUERIES[\"query.query_id\"].astype(int) == query_id].iloc[0]\n",
    "\n",
    "\n",
    "def get_queries_by_id(query_ids: List[int]) -> pd.DataFrame:\n",
    "    return QUERIES[QUERIES[\"query.query_id\"].astype(int).isin(query_ids)]\n",
    "\n",
    "\n",
    "def get_doc_by_id(doc_id: int) -> pd.Series:\n",
    "    return DOCUMENTS[DOCUMENTS[\"doc_id\"].astype(int) == doc_id].iloc[0]\n",
    "\n",
    "\n",
    "def openai_interface(system_prompt, user_prompt, response_format_pydantic=FactualQuestionResponse):\n",
    "    \"\"\"execute openai LLM call\"\"\"\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        response_format=response_format_pydantic,\n",
    "        temperature=0,\n",
    "    ).choices[0]\n",
    "\n",
    "    return completion.message.parsed\n",
    "\n",
    "\n",
    "def get_prompts_for_textual_manipulation(doc_id: int, query_id: int) -> Tuple:\n",
    "    PROMPT_TYPE = \"manipulation_factual\"\n",
    "    prompt = [\n",
    "        prompt for prompt in read_prompt(\"prompts/json/manipulate_docs.json\") if prompt[\"prompt_type\"] == PROMPT_TYPE\n",
    "    ][0]\n",
    "    text = get_doc_text(doc_id)\n",
    "    answer, keypoints, references, question = get_query_properties(query_id)\n",
    "    system_prompt = prompt[\"system_prompt\"]\n",
    "    user_prompt = format_user_prompt_textual(\n",
    "        user_prompt=prompt[\"user_prompt\"], text=text, answer=answer, question=question, references=references\n",
    "    )\n",
    "    return (system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "def get_prompts_for_tabular_manipulation(query_id: int, doc_id: int) -> Tuple:\n",
    "    prompt_obj = read_prompt(\"prompts/json/manipulation_tabular.json\")\n",
    "    system_prompt = prompt_obj[\"system_prompt\"]\n",
    "    user_prompt = prompt_obj[\"user_prompt\"]\n",
    "    answer, question = get_query_properties(query_id, properties=[\"ground_truth.content\", \"query.content\"])\n",
    "    (entity,) = get_doc_properties(doc_id, [\"hospital_patient_name\", \"company_name\", \"court_name\"])\n",
    "    user_prompt = format_user_prompt_tabular(user_prompt, question, answer, entity)\n",
    "    return (system_prompt, user_prompt)\n",
    "\n",
    "\n",
    "def get_id_for_manipulated_doc_or_query(original_doc_id: int, prefix_number=1) -> int:\n",
    "    id_str = str(prefix_number) + str(original_doc_id).zfill(5)\n",
    "    return int(id_str)\n",
    "\n",
    "\n",
    "def save_manipulated_doc(filename: os.PathLike | str, fieldnames: List[str], **kwargs):\n",
    "    \"\"\"Saves a manipulated doc to csv.\n",
    "    If an entry with that doc_id already exists in the csv, the new entry is NOT saved.\n",
    "    \"\"\"\n",
    "\n",
    "    is_empty = not os.path.exists(filename) or os.stat(filename).st_size == 0\n",
    "    id_exists = False\n",
    "    with open(filename, \"a+\", newline=\"\") as f:\n",
    "        if not is_empty:\n",
    "            f.seek(0)\n",
    "            reader = csv.DictReader(f, fieldnames=fieldnames)\n",
    "            ids_present = {int(row[\"doc_id\"]) for row in list(reader)[1:]}\n",
    "            id_exists = int(kwargs[\"doc_id\"]) in ids_present\n",
    "\n",
    "        if id_exists == True:\n",
    "            print(f\"WARN: Row with ID {kwargs[\"doc_id\"]} already exists. Did not write new document to '{filename}'.\")\n",
    "            return\n",
    "\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        if is_empty:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(kwargs)\n",
    "\n",
    "\n",
    "def save_manipulated_query(\n",
    "    filename: os.PathLike | str,\n",
    "    fieldnames: List[str],\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Saves a manipulated doc to csv. Adds column \"original_doc_id.\n",
    "    If an entry with that doc_id already exists in the csv, the new entry is NOT saved.\n",
    "    \"\"\"\n",
    "    is_empty = not os.path.exists(filename) or os.stat(filename).st_size == 0\n",
    "    id_exists = False\n",
    "    with open(filename, \"a+\", newline=\"\") as f:\n",
    "        if not is_empty:\n",
    "            f.seek(0)\n",
    "            reader = csv.DictReader(f, fieldnames=fieldnames)\n",
    "            ids_present = {int(row[\"query.query_id\"]) for row in list(reader)[1:]}\n",
    "            id_exists = int(kwargs[\"query.query_id\"]) in ids_present\n",
    "\n",
    "        if id_exists == True:\n",
    "            print(f\"WARN: Row with ID {kwargs[\"query.query_id\"]} already exists. Did not write new query to '{filename}'.\")\n",
    "            return\n",
    "\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        if is_empty:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ef2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:\n",
      " Index(['hospital_patient_name', 'language', 'doc_id', 'domain', 'content',\n",
      "       'company_name', 'court_name'],\n",
      "      dtype='object')\n",
      "\n",
      "Queries:\n",
      " Index(['domain', 'ground_truth.content', 'ground_truth.doc_ids',\n",
      "       'ground_truth.keypoints', 'ground_truth.references', 'language',\n",
      "       'prediction', 'query.content', 'query.query_id', 'query.query_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### describe documents and queries\n",
    "\n",
    "print(\"Documents:\\n\", DOCUMENTS.columns)\n",
    "print()\n",
    "print(\"Queries:\\n\", QUERIES.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53a2e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Did nothing.\n"
     ]
    }
   ],
   "source": [
    "### Randomly select documents from corpus\n",
    "import random\n",
    "import csv\n",
    "\n",
    "documents = pd.read_csv(\"data/DRAGONball/en/docs.csv\")\n",
    "\n",
    "ids_by_domain = {}\n",
    "\n",
    "\n",
    "for row in documents.itertuples(index=False):\n",
    "    new_list = ids_by_domain.get(row.domain, [])\n",
    "    new_list.append(row.doc_id)\n",
    "    ids_by_domain[row.domain] = new_list\n",
    "\n",
    "try:\n",
    "    with open(\"docs_to_manipulate.csv\", \"x\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        all_rand_ids = []\n",
    "\n",
    "        for domain, id_list in ids_by_domain.items():\n",
    "            print(f\"{domain}: {len(id_list)}\")\n",
    "            rand_ids = random.sample(id_list, k=10)\n",
    "            print(f\"\\t{rand_ids}\")\n",
    "            all_rand_ids += rand_ids\n",
    "\n",
    "        writer = csv.writer(f)\n",
    "        header = [\"doc_id\"]\n",
    "        writer.writerow(header)\n",
    "        for id in all_rand_ids:\n",
    "            writer.writerow([id])\n",
    "except FileExistsError:\n",
    "    print(\"File exists. Did nothing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d2f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Describe documents to manipulate\n",
    "with open(\"docs_to_manipulate.csv\", \"r\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # skip header\n",
    "    ids = set()\n",
    "    for row in reader:\n",
    "        ids.add(int(row[0]))\n",
    "\n",
    "documents_to_manipulate = DOCUMENTS[DOCUMENTS[\"doc_id\"].isin(ids)]\n",
    "# documents_to_manipulate[[\"doc_id\", \"domain\", \"company_name\", \"court_name\", \"hospital_patient_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Did nothing.\n"
     ]
    }
   ],
   "source": [
    "### -DEPRECATED- Create doc-query-query mapping\n",
    "try:\n",
    "    if os.path.exists(\"doc_query_mapping_multi.csv\"):\n",
    "        raise FileExistsError\n",
    "    with open(\"doc_query_mapping.csv\", \"r\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f, fieldnames=[\"doc_id\", \"query_id\"])\n",
    "        next(reader)  # skip headers+\n",
    "        doc_queries_mapping = []\n",
    "        for row in reader:\n",
    "            doc_id = int(row[\"doc_id\"])\n",
    "            query_id = int(row[\"query_id\"])\n",
    "            additional_query_id = int(query_id)\n",
    "\n",
    "            options = get_query_ids_for_doc(doc_id, query_types=[\"Factual Question\"])\n",
    "            while additional_query_id == query_id:\n",
    "                if len(options) <= 1:\n",
    "                    print(f\"WARN: For doc '{doc_id}', only {len(options)} options are available.\")\n",
    "                    break\n",
    "                additional_query_id = random.sample(options, 1)[0]\n",
    "            keys = [\"doc_id\", \"query_id_single\", \"query_id_multi\"]\n",
    "            values = [doc_id, query_id, additional_query_id]\n",
    "            doc_queries_mapping.append(dict(zip(keys, values)))\n",
    "            doc_queries_mapping.sort(key=lambda x: x[\"doc_id\"])\n",
    "\n",
    "    with open(\"doc_query_mapping_multi.csv\", \"x\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(doc_queries_mapping)\n",
    "except FileExistsError:\n",
    "    print(\"File exists. Did nothing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Did nothing.\n"
     ]
    }
   ],
   "source": [
    "### -DEPRECATED- Select a single query per document of type \"Factual Question\"\n",
    "\n",
    "\n",
    "doc_ids_for_man = set()\n",
    "\n",
    "with open(\"docs_to_manipulate.csv\", \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()\n",
    "\n",
    "    for row in reader:\n",
    "        doc_ids_for_man.add(int(row[0]))\n",
    "\n",
    "doc_query_mapping = []\n",
    "\n",
    "for doc_id in doc_ids_for_man:\n",
    "    factual_questions = get_query_ids_for_doc(doc_id, query_types=[\"Factual Question\"])\n",
    "    doc_query_mapping.append({\"doc_id\": doc_id, \"query_id\": random.sample(factual_questions, 1)[0]})\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(\"doc_query_mapping.csv\", \"x\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=doc_query_mapping[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(doc_query_mapping)\n",
    "except FileExistsError:\n",
    "    print(\"File exists. Did nothing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477d7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Row with ID 104620 already exists. Did not write new query to 'data/additional_data/queries/fact_single_manipulations.csv'.\n",
      "Finished processing doc 134 and query 4620.\n"
     ]
    }
   ],
   "source": [
    "### manipulate documents (textual)\n",
    "def save_doc_and_query(doc_entry, query_entry, completion_parsed):\n",
    "    # -- doc\n",
    "    original_doc_id = doc_entry.doc_id\n",
    "    manipulated_doc_entry = doc_entry.copy()\n",
    "    manipulated_doc_entry.doc_id = get_id_for_manipulated_doc_or_query(original_doc_id)\n",
    "    manipulated_doc_entry.content = completion_parsed.text_new\n",
    "    manipulated_doc_entry = pd.concat([manipulated_doc_entry, pd.Series([original_doc_id], index=[\"original_doc_id\"])])\n",
    "\n",
    "    save_manipulated_doc(**manipulated_doc_entry)\n",
    "\n",
    "    # -- query\n",
    "    original_query_id = query_entry[\"query.query_id\"]\n",
    "    manipulated_query_entry = query_entry.copy()\n",
    "    manipulated_query_entry[\"ground_truth.content\"] = completion_parsed.answer_new\n",
    "    manipulated_query_entry[\"ground_truth.references\"] = completion_parsed.references_new\n",
    "    manipulated_query_entry[\"ground_truth.keypoints\"] = []\n",
    "    manipulated_query_entry[\"query.query_id\"] = get_id_for_manipulated_doc_or_query(original_query_id)\n",
    "    manipulated_query_entry = pd.concat(\n",
    "        [manipulated_query_entry, pd.Series([original_query_id], index=[\"query.original_query_id\"])]\n",
    "    )\n",
    "    manipulated_query_entry.index = manipulated_query_entry.index.str.replace(\".\", \"__\", regex=False)\n",
    "\n",
    "    save_manipulated_query(**manipulated_query_entry)\n",
    "\n",
    "\n",
    "doc_query_mapping = get_doc_query_mapping()\n",
    "\n",
    "for mapping in doc_query_mapping:\n",
    "    doc_id = mapping[\"doc_id\"]\n",
    "    query_id = mapping[\"query_id\"]\n",
    "\n",
    "    doc_entry = get_doc_by_id(doc_id)\n",
    "    query_entry = get_query_by_id(query_id)\n",
    "\n",
    "    system_prompt, user_prompt = get_prompts_for_textual_manipulation(doc_id, query_id)\n",
    "\n",
    "    # call openai\n",
    "    completion_parsed = openai_interface(system_prompt, user_prompt)\n",
    "\n",
    "    save_doc_and_query(doc_entry, query_entry, completion_parsed)\n",
    "    print(f\"Finished processing doc {doc_id} and query {query_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f5ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200134, 'domain': 'Law', 'content': 'Chief judge according to the court judgment of Danbury, Pinehurst, Court | J. Smith', 'company_name': nan, 'court_name': 'Danbury, Pinehurst, Court', 'original_doc_id': 134, 'query.original_query_id': 4622, 'ground_truth.content': 'J. Smith'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200136, 'domain': 'Law', 'content': 'Residence of F. Williams according to the court judgment of Upton, Georgetown, Court | 45, Maple Avenue, Georgetown.', 'company_name': nan, 'court_name': 'Upton, Georgetown, Court', 'original_doc_id': 136, 'query.original_query_id': 4609, 'ground_truth.content': '45, Maple Avenue, Georgetown.'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200139, 'domain': 'Law', 'content': 'Defense lawyer for Y. Nelson according to the judgment of Glenwood, Quailwood, Court | J. Smith', 'company_name': nan, 'court_name': 'Glenwood, Quailwood, Court', 'original_doc_id': 139, 'query.original_query_id': 4644, 'ground_truth.content': 'J. Smith'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200046, 'domain': 'Finance', 'content': 'Changes that occurred in senior management of JetWing Aviation in October 2021 | Restructuring of senior management roles.', 'company_name': 'JetWing Aviation', 'court_name': nan, 'original_doc_id': 46, 'query.original_query_id': 2842, 'ground_truth.content': 'Restructuring of senior management roles.'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200047, 'domain': 'Finance', 'content': 'Person who resigned from the board of directors of CleanCo Housekeeping Services in November 2018 | Mr. John Smith', 'company_name': 'CleanCo Housekeeping Services', 'court_name': nan, 'original_doc_id': 47, 'query.original_query_id': 2247, 'ground_truth.content': 'Mr. John Smith'}\n",
      "{'hospital_patient_name': 'Parker General Hospital_Y. Evans', 'language': 'en', 'doc_id': 200179, 'domain': 'Medical', 'content': 'Past disease history of Y. Evans according to the hospitalization records of Parker General Hospital | Diabetes diagnosed 3 years ago, managed with lifestyle changes.', 'company_name': nan, 'court_name': nan, 'original_doc_id': 179, 'query.original_query_id': 5960, 'ground_truth.content': 'Diabetes diagnosed 3 years ago, managed with lifestyle changes.'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200052, 'domain': 'Finance', 'content': 'Amount by which EduCorp reduced its liabilities during the debt restructuring in March 2020 | $8 million', 'company_name': 'EduCorp', 'court_name': nan, 'original_doc_id': 52, 'query.original_query_id': 2525, 'ground_truth.content': '$8 million'}\n",
      "{'hospital_patient_name': 'Farmington General Hospital_J. Brooks', 'language': 'en', 'doc_id': 200181, 'domain': 'Medical', 'content': 'Age of J. Brooks according to the hospitalization records of Farmington General Hospital | 62', 'company_name': nan, 'court_name': nan, 'original_doc_id': 181, 'query.original_query_id': 5974, 'ground_truth.content': '62'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200059, 'domain': 'Finance', 'content': 'Month and year when Retail Emporium was established | March 2006', 'company_name': 'Retail Emporium', 'court_name': nan, 'original_doc_id': 59, 'query.original_query_id': 2696, 'ground_truth.content': 'March 2006'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200066, 'domain': 'Finance', 'content': 'Percentage of equity acquired by Buildcorp Holdings in A&B Construction | 40%', 'company_name': 'Buildcorp Holdings', 'court_name': nan, 'original_doc_id': 66, 'query.original_query_id': 2936, 'ground_truth.content': '40%'}\n",
      "{'hospital_patient_name': 'Southport General Hospital_K. Mendoza', 'language': 'en', 'doc_id': 200198, 'domain': 'Medical', 'content': 'Chief complaint of K. Mendoza according to the hospitalization records of Southport General Hospital | Severe headaches and dizziness for 3 months.', 'company_name': nan, 'court_name': nan, 'original_doc_id': 198, 'query.original_query_id': 6140, 'ground_truth.content': 'Severe headaches and dizziness for 3 months.'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200071, 'domain': 'Finance', 'content': 'Person added to the Board of Directors of ABC Education Corporation in August 2021 | Ms. Jane Doe', 'company_name': 'ABC Education Corporation', 'court_name': nan, 'original_doc_id': 71, 'query.original_query_id': 2540, 'ground_truth.content': 'Ms. Jane Doe'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200072, 'domain': 'Finance', 'content': 'Company acquired by MediaCorp in April 2018 | Creative Studios', 'company_name': 'MediaCorp', 'court_name': nan, 'original_doc_id': 72, 'query.original_query_id': 3186, 'ground_truth.content': 'Creative Studios'}\n",
      "{'hospital_patient_name': 'Wilton General Hospital_H. Flores', 'language': 'en', 'doc_id': 200199, 'domain': 'Medical', 'content': 'Initial diagnosis of H. Flores according to the hospitalization records of Wilton General Hospital | Viral Encephalitis', 'company_name': nan, 'court_name': nan, 'original_doc_id': 199, 'query.original_query_id': 6148, 'ground_truth.content': 'Viral Encephalitis'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200077, 'domain': 'Finance', 'content': 'Percentage of SunPower Corp acquired by Energen Solutions Ltd in June 2019 | 45%', 'company_name': 'Energen Solutions Ltd', 'court_name': nan, 'original_doc_id': 77, 'query.original_query_id': 2396, 'ground_truth.content': '45%'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200078, 'domain': 'Finance', 'content': 'Significant corporate event completed by InnovateTech, Inc. in November 2019 | Merger with TechFusion Ltd.', 'company_name': 'InnovateTech, Inc.', 'court_name': nan, 'original_doc_id': 78, 'query.original_query_id': 2818, 'ground_truth.content': 'Merger with TechFusion Ltd.'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200079, 'domain': 'Finance', 'content': 'Month, in which Elevate Retail Inc. invested in renewable energy projects | August 2021', 'company_name': 'Elevate Retail Inc.', 'court_name': nan, 'original_doc_id': 79, 'query.original_query_id': 2711, 'ground_truth.content': 'August 2021'}\n",
      "{'hospital_patient_name': 'Newport General Hospital_V. Lewis', 'language': 'en', 'doc_id': 200208, 'domain': 'Medical', 'content': 'Duration for which V. Lewis has had hypertension according to the hospitalization records of Newport General Hospital | 15 years', 'company_name': nan, 'court_name': nan, 'original_doc_id': 208, 'query.original_query_id': 6227, 'ground_truth.content': '15 years'}\n",
      "{'hospital_patient_name': 'Knoxville City Hospital_S. Moore', 'language': 'en', 'doc_id': 200209, 'domain': 'Medical', 'content': 'Ethnicity of S. Moore according to the hospitalization records of Knoxville City Hospital | Hispanic', 'company_name': nan, 'court_name': nan, 'original_doc_id': 209, 'query.original_query_id': 6233, 'ground_truth.content': 'Hispanic'}\n",
      "{'hospital_patient_name': 'Kingsport Medical Center_E. Chavez', 'language': 'en', 'doc_id': 200207, 'domain': 'Medical', 'content': 'Occupation of E. Chavez according to the hospitalization records of Kingsport Medical Center | Engineer', 'company_name': nan, 'court_name': nan, 'original_doc_id': 207, 'query.original_query_id': 6216, 'ground_truth.content': 'Engineer'}\n",
      "{'hospital_patient_name': 'Oxford General Hospital_G. Gonzalez', 'language': 'en', 'doc_id': 200205, 'domain': 'Medical', 'content': 'Temperature of G. Gonzalez during the physical examination according to the hospitalization records of Oxford General Hospital | 38.2°C', 'company_name': nan, 'court_name': nan, 'original_doc_id': 205, 'query.original_query_id': 6190, 'ground_truth.content': '38.2°C'}\n",
      "{'hospital_patient_name': 'Bridgewater General Hospital_J. Reyes', 'language': 'en', 'doc_id': 200212, 'domain': 'Medical', 'content': 'Chief complaint of J. Reyes according to the hospitalization records of Bridgewater General Hospital | Severe headaches and dizziness for 3 months.', 'company_name': nan, 'court_name': nan, 'original_doc_id': 212, 'query.original_query_id': 6266, 'ground_truth.content': 'Severe headaches and dizziness for 3 months.'}\n",
      "{'hospital_patient_name': 'Tremont City Hospital_X. Price', 'language': 'en', 'doc_id': 200211, 'domain': 'Medical', 'content': 'Admission time for X. Price according to the hospitalization records of Tremont City Hospital | 15th, February', 'company_name': nan, 'court_name': nan, 'original_doc_id': 211, 'query.original_query_id': 6241, 'ground_truth.content': '15th, February'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200112, 'domain': 'Law', 'content': 'Date of the court judgment of Bayside, Roseville, Court | 15th November, 2022', 'company_name': nan, 'court_name': 'Bayside, Roseville, Court', 'original_doc_id': 112, 'query.original_query_id': 4372, 'ground_truth.content': '15th November, 2022'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200114, 'domain': 'Law', 'content': 'Name of the chief judge according to the court judgment of Trenton, Eastwood, Court | J. Smith', 'company_name': nan, 'court_name': 'Trenton, Eastwood, Court', 'original_doc_id': 114, 'query.original_query_id': 4390, 'ground_truth.content': 'J. Smith'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200115, 'domain': 'Law', 'content': 'Location where J. Hall had a heated argument with R. Johnson according to the judgment of Urbana, Belmont, Court | Central Park', 'company_name': nan, 'court_name': 'Urbana, Belmont, Court', 'original_doc_id': 115, 'query.original_query_id': 4400, 'ground_truth.content': 'Central Park'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200119, 'domain': 'Law', 'content': 'Sentence for H. Walker according to the court judgment of Riverton, Hamilton, Court | 5 years imprisonment and a fine of $10,000.', 'company_name': nan, 'court_name': 'Riverton, Hamilton, Court', 'original_doc_id': 119, 'query.original_query_id': 4465, 'ground_truth.content': '5 years imprisonment and a fine of $10,000.'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200123, 'domain': 'Law', 'content': 'Fine imposed on Z. Torres according to the court judgment of Sterling, Quarryville, Court | $150,000', 'company_name': nan, 'court_name': 'Sterling, Quarryville, Court', 'original_doc_id': 123, 'query.original_query_id': 4481, 'ground_truth.content': '$150,000'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200125, 'domain': 'Law', 'content': 'Date of the court judgment of Hartford, Ashland, Court | 20th September 2022', 'company_name': nan, 'court_name': 'Hartford, Ashland, Court', 'original_doc_id': 125, 'query.original_query_id': 4531, 'ground_truth.content': '20th September 2022'}\n",
      "{'hospital_patient_name': nan, 'language': 'en', 'doc_id': 200127, 'domain': 'Law', 'content': 'Chief judge according to the court judgment of Hamilton, Harrison, Court | J. Smith', 'company_name': nan, 'court_name': 'Hamilton, Harrison, Court', 'original_doc_id': 127, 'query.original_query_id': 4522, 'ground_truth.content': 'J. Smith'}\n"
     ]
    }
   ],
   "source": [
    "### manipulate documents (tabular) and save rows\n",
    "mapping = get_doc_query_mapping(\"tabular\")\n",
    "\n",
    "doc_entries = []\n",
    "\n",
    "for id_pair in mapping:\n",
    "    doc_id = id_pair[\"doc_id\"]\n",
    "    query_id = id_pair[\"query_id\"]\n",
    "    system_prompt, user_prompt = get_prompts_for_tabular_manipulation(query_id, doc_id)\n",
    "\n",
    "    response: TabularDataResponse = openai_interface(system_prompt, user_prompt, TabularDataResponse)\n",
    "\n",
    "    manipulated_doc_entry = get_doc_by_id(doc_id).copy()\n",
    "    manipulated_doc_entry.doc_id = get_id_for_manipulated_doc_or_query(doc_id, prefix_number=2)\n",
    "    manipulated_doc_entry.content = \" | \".join([response.description, response.value])\n",
    "    additional_fields = pd.Series(\n",
    "        [doc_id, query_id, response.answer_new],\n",
    "        index=[\"original_doc_id\", \"query.original_query_id\", \"ground_truth.content\"],\n",
    "    )\n",
    "    manipulated_doc_entry = pd.concat([manipulated_doc_entry, additional_fields])\n",
    "\n",
    "    save_manipulated_doc(\n",
    "        filename=\"additional_data/docs/tabular_manipulations_result_rows.csv\",\n",
    "        fieldnames=COLUMNS_DOCS_MANIPULATED_TABULAR_ROW,\n",
    "        **manipulated_doc_entry,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa5280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Row with ID 300001 already exists. Did not write new document to 'data/additional_data/docs/tabular_manipulations_result.csv'.\n",
      "WARN: Row with ID 300002 already exists. Did not write new document to 'data/additional_data/docs/tabular_manipulations_result.csv'.\n",
      "WARN: Row with ID 300003 already exists. Did not write new document to 'data/additional_data/docs/tabular_manipulations_result.csv'.\n"
     ]
    }
   ],
   "source": [
    "### aggregate tabular rows and save docs\n",
    "tabular_docs = pd.read_csv(\"additional_data/docs/tabular_manipulations_result_rows.csv\")\n",
    "\n",
    "\n",
    "def list_or_none(series):\n",
    "    if series.dropna().empty:\n",
    "        return None\n",
    "    return series.tolist()\n",
    "\n",
    "\n",
    "agg_funcs = {\n",
    "    \"doc_id\": lambda x: 0,\n",
    "    \"language\": \"first\",\n",
    "    \"content\": \"\\n\".join,\n",
    "    \"company_name\": list_or_none,\n",
    "    \"court_name\": list_or_none,\n",
    "    \"hospital_patient_name\": list_or_none,\n",
    "    \"original_doc_id\": list_or_none,\n",
    "    \"query.original_query_id\": lambda x: None,\n",
    "    \"ground_truth.content\": lambda x: None,\n",
    "}\n",
    "\n",
    "aggregation = tabular_docs.groupby(\"domain\").agg(agg_funcs).reset_index()\n",
    "aggregation[\"doc_id\"] = [get_id_for_manipulated_doc_or_query(id, prefix_number=3) for id in [1, 2, 3]]\n",
    "aggregation.rename(\n",
    "    columns={\n",
    "        \"company_name\": \"company_names\",\n",
    "        \"court_name\": \"court_names\",\n",
    "        \"hospital_patient_name\": \"hospital_patient_names\",\n",
    "        \"original_doc_id\": \"original_doc_ids\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "for row_dict in aggregation.to_dict(orient=\"records\"):\n",
    "    save_manipulated_doc(\n",
    "        \"additional_data/docs/tabular_manipulations_result.csv\",\n",
    "        fieldnames=COLUMNS_DOCS_MANIPULATED_TABULAR,\n",
    "        **row_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d16476",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save manipulated queries for aggregated tabular docs\n",
    "tabular_docs = pd.read_csv(\"additional_data/docs/tabular_manipulations_result_rows.csv\")\n",
    "\n",
    "mapping_domain_doc_id = pd.read_csv(\n",
    "    \"additional_data/docs/tabular_manipulations_result.csv\", usecols=[\"domain\", \"doc_id\"]\n",
    ")\n",
    "mapping_dict = mapping_domain_doc_id.set_index(\"domain\")[\"doc_id\"].to_dict()\n",
    "\n",
    "for row_dict in tabular_docs.to_dict(orient=\"records\"):\n",
    "    original_query_id = row_dict[\"query.original_query_id\"]\n",
    "    manipulated_query_entry = get_query_by_id(original_query_id).copy()\n",
    "\n",
    "    manipulated_query_entry[\"ground_truth.doc_ids\"] = [mapping_dict[row_dict[\"domain\"]]]\n",
    "    manipulated_query_entry[\"ground_truth.content\"] = row_dict[\"ground_truth.content\"]\n",
    "    manipulated_query_entry[\"ground_truth.references\"] = row_dict[\"content\"]\n",
    "    manipulated_query_entry[\"ground_truth.keypoints\"] = []\n",
    "    manipulated_query_entry[\"query.query_id\"] = get_id_for_manipulated_doc_or_query(original_query_id, prefix_number=3)\n",
    "    manipulated_query_entry = pd.concat(\n",
    "        [manipulated_query_entry, pd.Series([original_query_id], index=[\"query.original_query_id\"])]\n",
    "    )\n",
    "\n",
    "    save_manipulated_query(\n",
    "        filename=\"additional_data/queries/tabular_manipulations_result.csv\",\n",
    "        fieldnames=COLUMNS_QUERIES_MANIPULATED,\n",
    "        **manipulated_query_entry\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
